from flask import Blueprint, request, jsonify, Response
import json
import uuid
import requests
from services.ai_service import ai_service
from services.web_crawler import web_crawler
from services.ocr_service import ocr_service
from utils.file_handler import save_uploaded_files, extract_text_from_file

comprehensive_analysis_bp = Blueprint('comprehensive_analysis', __name__)

SEARCH_API_URL = "http://chengyuxuan.top:3100/search"

def search_information(query):
    """æœç´¢ç›¸å…³ä¿¡æ¯"""
    try:
        params = {
            'q': query,
            'format': 'json'
        }
        response = requests.get(SEARCH_API_URL, params=params, timeout=30)
        response.raise_for_status()
        
        data = response.json()
        return data
        
    except Exception as e:
        return {'error': str(e)}

@comprehensive_analysis_bp.route('/start', methods=['POST'])
def start_comprehensive_analysis():
    """å¼€å§‹å…¨é¢æ€»ç»“åˆ†æ"""
    try:
        # è·å–ä¸Šä¼ çš„å†…å®¹
        content_type = request.form.get('content_type')
        content_data = request.form.get('content_data', '')
        
        extracted_content = ""
        
        # æå–å†…å®¹ï¼ˆä¸æ™ºèƒ½ä¼´è¯»ç›¸åŒçš„é€»è¾‘ï¼‰
        if content_type == 'url':
            urls = [url.strip() for url in content_data.split('\n') if url.strip()]
            if not urls:
                return jsonify({'success': False, 'message': 'è¯·æä¾›æœ‰æ•ˆçš„ç½‘å€'})
            
            print(f"=== URLæå–è°ƒè¯• ===")
            print(f"URLs: {urls}")
            
            results = web_crawler.extract_text_from_multiple_urls(urls)
            
            print(f"çˆ¬è™«ç»“æœæ•°é‡: {len(results)}")
            for i, result in enumerate(results):
                print(f"ç»“æœ {i}: success={result.get('success')}, content_length={len(result.get('content', ''))}")
            
            successful_extractions = 0
            for result in results:
                if result['success']:
                    extracted_content += f"\n\n=== {result['title']} ===\n{result['content']}"
                    successful_extractions += 1
                    print(f"æˆåŠŸæå–: {result['title'][:50]}...")
                else:
                    extracted_content += f"\n\né”™è¯¯ï¼šæ— æ³•æå– {result['url']} çš„å†…å®¹ï¼š{result['error']}"
                    print(f"æå–å¤±è´¥: {result['url']} - {result['error']}")
            
            print(f"æ€»æå–å†…å®¹é•¿åº¦: {len(extracted_content)}")
            print(f"================")
            
            # å¦‚æœæ‰€æœ‰URLéƒ½å¤±è´¥äº†ï¼Œè¿”å›é”™è¯¯
            if successful_extractions == 0:
                return jsonify({'success': False, 'message': 'æ— æ³•è·å–ä»»ä½•ç½‘é¡µå†…å®¹ï¼Œè¯·æ£€æŸ¥ç½‘å€æ˜¯å¦æœ‰æ•ˆæˆ–ç½‘ç»œè¿æ¥'})
        
        elif content_type == 'text':
            extracted_content = content_data
        
        elif content_type == 'file':
            files = request.files.getlist('files')
            if not files:
                return jsonify({'success': False, 'message': 'æœªæ£€æµ‹åˆ°ä¸Šä¼ çš„æ–‡ä»¶'})
                
            saved_paths = save_uploaded_files(files)
            
            for file_path in saved_paths:
                file_content = extract_text_from_file(file_path)
                if file_content:
                    extracted_content += f"\n\n=== {file_path} ===\n{file_content}"
        
        elif content_type == 'image':
            images = request.files.getlist('images')
            if not images:
                return jsonify({'success': False, 'message': 'æœªæ£€æµ‹åˆ°ä¸Šä¼ çš„å›¾ç‰‡'})
                
            saved_paths = ocr_service.save_uploaded_images(images)
            ocr_results = ocr_service.extract_text_from_multiple_images(saved_paths)
            
            for result in ocr_results:
                if result['success']:
                    extracted_content += f"\n\n=== å›¾ç‰‡æ–‡å­— ===\n{result['text']}"
                else:
                    extracted_content += f"\n\né”™è¯¯ï¼šæ— æ³•è¯†åˆ«å›¾ç‰‡æ–‡å­—ï¼š{result['error']}"
        
        if not extracted_content.strip():
            return jsonify({'success': False, 'message': 'æ²¡æœ‰æå–åˆ°ä»»ä½•æœ‰æ•ˆå†…å®¹ï¼Œè¯·æ£€æŸ¥ä¸Šä¼ çš„å†…å®¹'})
        
        # ç”Ÿæˆä¼šè¯ID
        session_id = str(uuid.uuid4())
        
        # ä½¿ç”¨æµå¼å“åº”å®ç°å®æ—¶è¿›åº¦æ›´æ–°
        return Response(
            generate_comprehensive_analysis(extracted_content, session_id),
            mimetype='text/plain'
        )
        
    except Exception as e:
        return jsonify({'success': False, 'message': f'å¼€å§‹å…¨é¢åˆ†æå¤±è´¥ï¼š{str(e)}'})

def generate_comprehensive_analysis(content, session_id):
    """ç”Ÿæˆå…¨é¢åˆ†æçš„å››ä¸ªæ­¥éª¤"""
    try:
        yield f"data: {json.dumps({'type': 'session_id', 'session_id': session_id})}\n\n"
        
        # è°ƒè¯•ä¿¡æ¯
        print(f"=== å…¨é¢åˆ†æè°ƒè¯•ï¼ˆæµå¼ç‰ˆæœ¬ï¼‰===")
        print(f"ä¼šè¯ID: {session_id}")
        print(f"å†…å®¹é•¿åº¦: {len(content)}")
        print(f"å†…å®¹å‰200å­—ç¬¦: {content[:200]}")
        print(f"==================")
        
        # æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºç©ºæˆ–åŒ…å«çˆ¬è™«é”™è¯¯
        if not content.strip():
            yield f"data: {json.dumps({'type': 'error', 'message': 'å†…å®¹ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œåˆ†æ'})}\n\n"
            return
            
        if "é”™è¯¯ï¼šæ— æ³•æå–" in content:
            yield f"data: {json.dumps({'type': 'error', 'message': 'ç½‘é¡µå†…å®¹æå–å¤±è´¥ï¼Œè¯·æ£€æŸ¥é“¾æ¥æ˜¯å¦æœ‰æ•ˆ'})}\n\n"
            return
            
        if "æ²¡æœ‰æå–åˆ°ä»»ä½•å†…å®¹" in content:
            yield f"data: {json.dumps({'type': 'error', 'message': 'æ²¡æœ‰æå–åˆ°ä»»ä½•æœ‰æ•ˆå†…å®¹ï¼Œè¯·é‡æ–°å°è¯•'})}\n\n"
            return
        
        # ç¬¬ä¸€æ­¥ï¼šæ–‡ç« æ¦‚è¦
        print("=" * 30)
        print("å¼€å§‹ç¬¬ä¸€æ­¥ï¼šæ–‡ç« æ¦‚è¦")
        print("=" * 30)
        yield f"data: {json.dumps({'type': 'step_start', 'step': 1, 'name': 'æ–‡ç« æ¦‚è¦', 'description': 'æå–æ–‡ç« å¤§æ„å’Œæ ¸å¿ƒä¿¡æ¯'})}\n\n"
        
        overview_prompt = f"""æˆ‘æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹åˆ†æåŠ©æ‰‹ã€‚è¯·å¯¹ä»¥ä¸‹å†…å®¹è¿›è¡Œå…¨é¢çš„æ¦‚è¦åˆ†æã€‚

**å¾…åˆ†æå†…å®¹ï¼š**
{content}

**åˆ†æä»»åŠ¡ï¼š**
è¯·å¯¹ä¸Šè¿°å†…å®¹è¿›è¡Œå…¨é¢çš„æ¦‚è¦åˆ†æï¼ŒåŒ…æ‹¬ï¼š

## ğŸ“‹ æ–‡ç« æ¦‚è¦åˆ†æ

### ğŸ¯ ä¸»è¦ä¸»é¢˜
- æ˜ç¡®è¯†åˆ«æ–‡ç« çš„æ ¸å¿ƒä¸»é¢˜
- åˆ†æä¸»é¢˜çš„é‡è¦æ€§å’Œç›¸å…³æ€§

### ğŸ’¡ æ ¸å¿ƒè§‚ç‚¹
- æå–æ–‡ç« çš„æ ¸å¿ƒè§‚ç‚¹å’Œä¸»è¦è®ºè¿°
- åˆ†æè§‚ç‚¹çš„é€»è¾‘æ€§å’Œè¯´æœåŠ›

### ğŸ“Š ä¸»è¦è®ºè¿°å†…å®¹
- **ä¸»è¦è®ºç‚¹**ï¼šç³»ç»Ÿæ¢³ç†æ–‡ç« çš„ä¸»è¦è®ºç‚¹
- **è®ºè¿°é€»è¾‘**ï¼šåˆ†æè®ºè¿°çš„é€»è¾‘å…³ç³»å’Œç»“æ„
- **æ”¯æ’‘è¯æ®**ï¼šè¯†åˆ«ç”¨äºæ”¯æ’‘è®ºç‚¹çš„è¯æ®å’Œæ¡ˆä¾‹

### ğŸ” å…³é”®ä¿¡æ¯ç‚¹
- **é‡è¦äº‹å®å’Œæ•°æ®**ï¼šæå–é‡è¦çš„äº‹å®ã€æ•°æ®ã€ç»Ÿè®¡ä¿¡æ¯
- **å…³é”®æœ¯è¯­å’Œæ¦‚å¿µ**ï¼šè¯†åˆ«å¹¶è§£é‡Šå…³é”®æœ¯è¯­å’Œæ¦‚å¿µ
- **é‡è¦äººç‰©å’Œæœºæ„**ï¼šè¯†åˆ«æ–‡ä¸­æåˆ°çš„é‡è¦äººç‰©å’Œæœºæ„

### ğŸ“– æ–‡ç« ç»“æ„
- åˆ†ææ–‡ç« çš„ç»„ç»‡ç»“æ„å’Œæ®µè½å®‰æ’
- è¯„ä¼°ç»“æ„çš„åˆç†æ€§å’Œé€»è¾‘æ€§

è¯·ä½¿ç”¨æ¸…æ™°çš„markdownæ ¼å¼ï¼Œç¡®ä¿åˆ†æå…¨é¢è€Œæ·±å…¥ã€‚

<think>
æˆ‘éœ€è¦ä»”ç»†é˜…è¯»è¿™ç¯‡å†…å®¹ï¼Œä»å¤šä¸ªç»´åº¦è¿›è¡Œæ¦‚è¦åˆ†æï¼ŒåŒ…æ‹¬ä¸»é¢˜ã€è§‚ç‚¹ã€è®ºè¿°ã€ä¿¡æ¯ç‚¹å’Œç»“æ„ç­‰æ–¹é¢ã€‚
</think>"""
        
        overview_content = ""
        print("æ­£åœ¨è°ƒç”¨AIè¿›è¡Œæ¦‚è¦åˆ†æ...")
        for chunk in ai_service.complex_chat(overview_prompt):
            thinking, display = ai_service.extract_thinking(chunk)
            if thinking:
                yield f"data: {json.dumps({'type': 'thinking', 'step': 1, 'content': thinking})}\n\n"
            if display:
                overview_content += display
                yield f"data: {json.dumps({'type': 'content', 'step': 1, 'content': display})}\n\n"
        
        print(f"æ¦‚è¦åˆ†æå®Œæˆï¼Œå†…å®¹é•¿åº¦: {len(overview_content)}")
        yield f"data: {json.dumps({'type': 'step_complete', 'step': 1})}\n\n"
        
        # ç¬¬äºŒæ­¥ï¼šæœç´¢ç»“æœ
        print("=" * 30)
        print("å¼€å§‹ç¬¬äºŒæ­¥ï¼šæœç´¢ç»“æœ")
        print("=" * 30)
        yield f"data: {json.dumps({'type': 'step_start', 'step': 2, 'name': 'æœç´¢ç»“æœ', 'description': 'æœç´¢ç›¸å…³èµ„æ–™å’Œä¿¡æ¯'})}\n\n"
        
        # é¦–å…ˆè®©AIæå–æœç´¢å…³é”®è¯
        yield f"data: {json.dumps({'type': 'thinking', 'step': 2, 'content': 'æ­£åœ¨åŸºäºæ–‡ç« å†…å®¹æå–æœç´¢å…³é”®è¯...'})}\n\n"
        
        keyword_prompt = f"""åŸºäºä»¥ä¸‹æ¦‚è¦åˆ†æç»“æœï¼š
{overview_content}

ä»¥åŠåŸæ–‡å†…å®¹ï¼š
{content}

è¯·æå–3-5ä¸ªæœ€é‡è¦çš„æœç´¢å…³é”®è¯ï¼Œç”¨äºæœç´¢ç›¸å…³èµ„æ–™å’Œä¿¡æ¯ã€‚
å…³é”®è¯åº”è¯¥æ˜¯ï¼š
1. æ–‡ç« çš„æ ¸å¿ƒä¸»é¢˜
2. é‡è¦çš„æ¦‚å¿µæˆ–æœ¯è¯­
3. å¯èƒ½éœ€è¦è¿›ä¸€æ­¥äº†è§£çš„è¯é¢˜
4. ç›¸å…³çš„äººç‰©ã€æœºæ„æˆ–äº‹ä»¶

è¯·ç›´æ¥è¾“å‡ºå…³é”®è¯ï¼Œæ¯è¡Œä¸€ä¸ªï¼Œä¸è¦å…¶ä»–æ ¼å¼å’Œè§£é‡Šã€‚"""
        
        keywords_content = ""
        for chunk in ai_service.simple_chat(keyword_prompt):
            thinking, display = ai_service.extract_thinking(chunk)
            if thinking:
                yield f"data: {json.dumps({'type': 'thinking', 'step': 2, 'content': thinking})}\n\n"
            if display:
                keywords_content += display
        
        # è§£æå…³é”®è¯
        keywords = [kw.strip() for kw in keywords_content.split('\n') if kw.strip() and not kw.startswith('#') and not kw.startswith('*')]
        keywords = keywords[:5]  # æœ€å¤š5ä¸ªå…³é”®è¯
        
        print(f"æå–åˆ°çš„å…³é”®è¯: {keywords}")
        
        # ç¾åŒ–å…³é”®è¯æ˜¾ç¤º
        keywords_display = "## ğŸ¯ æå–çš„æœç´¢å…³é”®è¯\\n\\n"
        for i, kw in enumerate(keywords, 1):
            keywords_display += f"**{i}.** `{kw}`\\n\\n"
        keywords_display += "\\næ­£åœ¨åŸºäºè¿™äº›å…³é”®è¯æœç´¢ç›¸å…³èµ„æ–™...\\n\\n"
        
        yield f"data: {json.dumps({'type': 'content', 'step': 2, 'content': keywords_display})}\n\n"
        
        # æ‰§è¡Œæœç´¢
        search_results = []
        search_results_data = []
        
        for keyword in keywords:
            print(f"æ­£åœ¨æœç´¢å…³é”®è¯: {keyword}")
            yield f"data: {json.dumps({'type': 'thinking', 'step': 2, 'content': f'æ­£åœ¨æœç´¢å…³é”®è¯: {keyword}'})}\n\n"
            
            search_result = search_information(keyword)
            if 'error' not in search_result and 'results' in search_result and search_result['results']:
                search_results.append({
                    'keyword': keyword,
                    'results': search_result
                })
                
                results = search_result['results'][:3]  # æ˜¾ç¤ºå‰3ä¸ªç»“æœ
                print(f"  æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç»“æœ")
                
                # æ˜¾ç¤ºæœç´¢ç»“æœ
                result_content = f"\\n\\n### ğŸ” å…³é”®è¯: {keyword}\\n\\n"
                
                for i, item in enumerate(results, 1):
                    title = item.get('title', 'æ— æ ‡é¢˜')
                    url = item.get('url', '')
                    snippet = item.get('snippet', item.get('description', 'æ— æè¿°'))
                    
                    # ä½¿ç”¨ç®€æ´çš„æ–‡æœ¬æ ¼å¼ï¼Œé¿å…å¤æ‚åµŒå¥—
                    result_content += f"{i}. **{title}**\\n"
                    if snippet and snippet != 'æ— æè¿°':
                        result_content += f"   ğŸ“„ {snippet}\\n"
                    if url:
                        result_content += f"   ğŸ”— {url}\\n"
                    result_content += "\\n"
                    
                    # ä¿å­˜æœç´¢ç»“æœæ•°æ®ä¾›åç»­åˆ†æä½¿ç”¨
                    search_results_data.append({
                        'keyword': keyword,
                        'title': title,
                        'snippet': snippet,
                        'url': url
                    })
                
                yield f"data: {json.dumps({'type': 'content', 'step': 2, 'content': result_content})}\n\n"
            else:
                print(f"  æœç´¢ '{keyword}' æ—¶å‡ºç°é”™è¯¯æˆ–æ— ç»“æœ")
                error_msg = search_result.get('error', 'æœªçŸ¥é”™è¯¯')
                yield f"data: {json.dumps({'type': 'thinking', 'step': 2, 'content': f'æœç´¢ {keyword} å¤±è´¥: {error_msg}'})}\n\n"
                
                result_content = f"\\n\\n### ğŸ” å…³é”®è¯: {keyword}\\n\\nâŒ æœç´¢æ—¶å‡ºç°é”™è¯¯æˆ–æ— ç»“æœ\\n\\n"
                yield f"data: {json.dumps({'type': 'content', 'step': 2, 'content': result_content})}\n\n"
        
        # æ€»ç»“æœç´¢ç»“æœ
        summary_content = f"\\n\\n## ğŸ“Š æœç´¢ç»“æœæ€»ç»“\\n\\n"
        summary_content += f"- æˆåŠŸæœç´¢å…³é”®è¯: {len(search_results)}/{len(keywords)}\\n"
        summary_content += f"- æ€»å…±è·å–åˆ° {len(search_results_data)} æ¡ç›¸å…³ä¿¡æ¯\\n"
        
        if search_results:
            summary_content += "\\nè¿™äº›æœç´¢ç»“æœä¸ºæ–‡ç« åˆ†ææä¾›äº†é¢å¤–çš„èƒŒæ™¯ä¿¡æ¯å’Œç›¸å…³èµ„æ–™ï¼Œæœ‰åŠ©äºæ›´æ·±å…¥åœ°ç†è§£æ–‡ç« å†…å®¹ã€‚"
        
        yield f"data: {json.dumps({'type': 'content', 'step': 2, 'content': summary_content})}\n\n"
        print(f"æœç´¢ç»“æœæ±‡æ€»å®Œæˆï¼Œå…±è·å¾— {len(search_results_data)} æ¡æœ‰æ•ˆç»“æœ")
        
        yield f"data: {json.dumps({'type': 'step_complete', 'step': 2})}\n\n"
        
        # ç¬¬ä¸‰æ­¥ï¼šæ·±å…¥æ€è€ƒ
        print("=" * 30)
        print("å¼€å§‹ç¬¬ä¸‰æ­¥ï¼šæ·±å…¥æ€è€ƒ")
        print("=" * 30)
        yield f"data: {json.dumps({'type': 'step_start', 'step': 3, 'name': 'æ·±å…¥æ€è€ƒ', 'description': 'ç»“åˆæœç´¢ç»“æœæ·±å…¥åˆ†ææ–‡ç« '})}\n\n"
        
        # å°†æœç´¢ç»“æœæ•´ç†ä¸ºæ–‡æœ¬
        search_context = ""
        if search_results_data:
            search_context = "ç›¸å…³æœç´¢ç»“æœï¼š\n"
            for result in search_results_data:
                search_context += f"- å…³é”®è¯'{result['keyword']}'ï¼š{result['title']} - {result['snippet']}\n"
        
        thinking_prompt = f"""åŸºäºå‰é¢çš„æ¦‚è¦åˆ†æï¼š
{overview_content}

ç»“åˆæœç´¢ç»“æœï¼š
{search_context}

è¯·å¯¹åŸæ–‡è¿›è¡Œæ·±å…¥æ€è€ƒå’Œåˆ†æï¼š

åŸæ–‡ï¼š
{content}

**åˆ†æä»»åŠ¡ï¼š**
è¯·ç»“åˆæœç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯ï¼Œå¯¹æ–‡ç« è¿›è¡Œæ·±åº¦åˆ†æï¼š

## ğŸ”¬ æ·±åº¦åˆ†æ

### ğŸ’ å†…å®¹ä»·å€¼è¯„ä¼°
- **å­¦æœ¯ä»·å€¼**ï¼šè¯„ä¼°å†…å®¹çš„å­¦æœ¯æ„ä¹‰å’Œç†è®ºè´¡çŒ®
- **å®ç”¨ä»·å€¼**ï¼šåˆ†æå†…å®¹çš„å®é™…åº”ç”¨ä»·å€¼å’ŒæŒ‡å¯¼æ„ä¹‰
- **ç¤¾ä¼šå½±å“**ï¼šè¯„ä¼°å†…å®¹å¯èƒ½äº§ç”Ÿçš„ç¤¾ä¼šå½±å“å’Œæ„ä¹‰
- **åˆ›æ–°æ€§**ï¼šåˆ†æå†…å®¹çš„åˆ›æ–°ç‚¹å’Œç‹¬ç‰¹ä¹‹å¤„

### ğŸ­ è§‚ç‚¹æ·±åº¦è§£è¯»
- **è§‚ç‚¹å±‚æ¬¡**ï¼šåˆ†æè§‚ç‚¹çš„æ·±åº¦å’Œå¤æ‚æ€§
- **è®ºè¯è´¨é‡**ï¼šè¯„ä¼°è®ºè¯çš„ä¸¥å¯†æ€§å’Œè¯´æœåŠ›
- **åè§è¯†åˆ«**ï¼šè¯†åˆ«å¯èƒ½å­˜åœ¨çš„åè§æˆ–å±€é™æ€§
- **å¤šè§’åº¦è§†è§’**ï¼šä»ä¸åŒè§’åº¦è§£è¯»æ ¸å¿ƒè§‚ç‚¹

### ğŸ”— é€»è¾‘å…³ç³»åˆ†æ
- **è®ºè¯ç»“æ„**ï¼šåˆ†æè®ºè¯çš„é€»è¾‘ç»“æ„å’Œæ¨ç†è¿‡ç¨‹
- **å› æœå…³ç³»**ï¼šè¯†åˆ«å†…å®¹ä¸­çš„å› æœå…³ç³»é“¾
- **çŸ›ç›¾å†²çª**ï¼šå‘ç°å¯èƒ½å­˜åœ¨çš„é€»è¾‘çŸ›ç›¾æˆ–å†²çª
- **è®ºè¯ç¼ºé™·**ï¼šæŒ‡å‡ºè®ºè¯ä¸­çš„è–„å¼±ç¯èŠ‚

### ğŸŒ èƒŒæ™¯å’Œä¸Šä¸‹æ–‡
- **å†å²èƒŒæ™¯**ï¼šåˆ†æå†…å®¹çš„å†å²èƒŒæ™¯å’Œå‘å±•è„‰ç»œ
- **ç¤¾ä¼šç¯å¢ƒ**ï¼šè€ƒè™‘å†…å®¹äº§ç”Ÿçš„ç¤¾ä¼šç¯å¢ƒå’Œæ¡ä»¶
- **ç›¸å…³ç ”ç©¶**ï¼šç»“åˆæœç´¢ç»“æœåˆ†æç›¸å…³ç ”ç©¶å’Œè§‚ç‚¹
- **æ—¶ä»£æ„ä¹‰**ï¼šè¯„ä¼°å†…å®¹åœ¨å½“å‰æ—¶ä»£çš„æ„ä¹‰å’Œä»·å€¼

### ğŸ¤” æ‰¹åˆ¤æ€§æ€è€ƒ
- **è´¨ç–‘ä¸åæ€**ï¼šå¯¹å†…å®¹è¿›è¡Œæ‰¹åˆ¤æ€§è´¨ç–‘å’Œåæ€
- **æ›¿ä»£è§‚ç‚¹**ï¼šæå‡ºå¯èƒ½çš„æ›¿ä»£è§‚ç‚¹æˆ–è§£é‡Š
- **æ”¹è¿›å»ºè®®**ï¼šæå‡ºå†…å®¹å¯ä»¥æ”¹è¿›çš„æ–¹é¢
- **æœªæ¥å±•æœ›**ï¼šåˆ†æå†…å®¹çš„å‘å±•è¶‹åŠ¿å’Œæœªæ¥å¯èƒ½æ€§

è¯·è¿›è¡Œæ·±å…¥è€Œå…¨é¢çš„åˆ†æï¼Œä½¿ç”¨æ¸…æ™°çš„markdownæ ¼å¼ã€‚

<think>
æˆ‘éœ€è¦ç»“åˆæ¦‚è¦åˆ†æå’Œæœç´¢ç»“æœï¼Œå¯¹åŸå§‹å†…å®¹è¿›è¡Œæ·±å…¥çš„æ‰¹åˆ¤æ€§åˆ†æï¼Œä»å¤šä¸ªç»´åº¦è¯„ä¼°å…¶ä»·å€¼ã€è§‚ç‚¹ã€é€»è¾‘å’Œæ„ä¹‰ã€‚
</think>"""
        
        deep_content = ""
        print("æ­£åœ¨è¿›è¡Œæ·±åº¦åˆ†æ...")
        for chunk in ai_service.complex_chat(thinking_prompt):
            thinking, display = ai_service.extract_thinking(chunk)
            if thinking:
                yield f"data: {json.dumps({'type': 'thinking', 'step': 3, 'content': thinking})}\n\n"
            if display:
                deep_content += display
                yield f"data: {json.dumps({'type': 'content', 'step': 3, 'content': display})}\n\n"
        
        print(f"æ·±åº¦åˆ†æå®Œæˆï¼Œå†…å®¹é•¿åº¦: {len(deep_content)}")
        yield f"data: {json.dumps({'type': 'step_complete', 'step': 3})}\n\n"
        
        # ç¬¬å››æ­¥ï¼šæ€»ç»“å½’çº³
        print("=" * 30)
        print("å¼€å§‹ç¬¬å››æ­¥ï¼šæ€»ç»“å½’çº³")
        print("=" * 30)
        yield f"data: {json.dumps({'type': 'step_start', 'step': 4, 'name': 'æ€»ç»“å½’çº³', 'description': 'ç»¼åˆæ‰€æœ‰åˆ†æç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š'})}\n\n"
        
        # é™åˆ¶å‰é¢æ­¥éª¤å†…å®¹çš„é•¿åº¦ï¼Œé¿å…promptè¿‡é•¿
        overview_summary = overview_content[:500] + "..." if len(overview_content) > 500 else overview_content
        search_summary = search_context[:300] + "..." if len(search_context) > 300 else search_context
        deep_summary = deep_content[:500] + "..." if len(deep_content) > 500 else deep_content
        
        print(f"ç¬¬å››æ­¥prompté•¿åº¦æ§åˆ¶ - æ¦‚è¦: {len(overview_summary)}, æœç´¢: {len(search_summary)}, æ·±åº¦: {len(deep_summary)}")
        
        # ç®€åŒ–promptï¼Œé¿å…è¿‡é•¿å¯¼è‡´AIæ— å“åº”
        summary_prompt = f"""åŸºäºå‰é¢çš„åˆ†æï¼Œè¯·ç”Ÿæˆæœ€ç»ˆçš„ç»¼åˆæ€»ç»“æŠ¥å‘Šã€‚

æ¦‚è¦åˆ†ææ‘˜è¦ï¼š
{overview_summary}

æœç´¢ç»“æœæ‘˜è¦ï¼š
{search_summary}

æ·±åº¦åˆ†ææ‘˜è¦ï¼š
{deep_summary}

è¯·ç”Ÿæˆï¼š

## ğŸ“Š æœ€ç»ˆåˆ†ææ€»ç»“

### ğŸ¯ æ ¸å¿ƒå‘ç°
æ€»ç»“æœ€é‡è¦çš„å‘ç°å’Œæ´å¯Ÿ

### ğŸ’ å…³é”®ä»·å€¼ç‚¹
- ç†è®ºä»·å€¼
- å®è·µä»·å€¼  
- å¯å‘ä»·å€¼

### ğŸ“‹ ç»¼åˆè¯„ä»·
- ä¼˜ç‚¹åˆ†æ
- ä¸è¶³ä¹‹å¤„
- æ•´ä½“è¯„ä¼°

### ğŸ’¡ å®ç”¨å»ºè®®
ä¸ºè¯»è€…æä¾›å®ç”¨çš„å»ºè®®å’Œå¯ç¤º

### ğŸ”® å»¶ä¼¸æ€è€ƒ
æå‡ºå€¼å¾—è¿›ä¸€æ­¥æ¢ç´¢çš„é—®é¢˜å’Œæ–¹å‘

è¯·ç”¨æ¸…æ™°çš„markdownæ ¼å¼ï¼Œæä¾›æœ‰ä»·å€¼çš„æ€»ç»“ã€‚

<think>
æˆ‘éœ€è¦åŸºäºå‰é¢çš„åˆ†æï¼Œç”Ÿæˆä¸€ä¸ªå®Œæ•´è€Œæœ‰ä»·å€¼çš„æœ€ç»ˆæ€»ç»“æŠ¥å‘Šã€‚
</think>"""
        
        print("æ­£åœ¨ç”Ÿæˆæœ€ç»ˆæ±‡æ€»...")
        summary_content = ""
        chunk_count = 0
        
        try:
            for chunk in ai_service.complex_chat(summary_prompt):
                chunk_count += 1
                print(f"ç¬¬å››æ­¥æ”¶åˆ°chunk {chunk_count}: {chunk[:100]}...")
                
                thinking, display = ai_service.extract_thinking(chunk)
                if thinking:
                    print(f"ç¬¬å››æ­¥æ€è€ƒå†…å®¹: {thinking[:100]}...")
                    yield f"data: {json.dumps({'type': 'thinking', 'step': 4, 'content': thinking})}\n\n"
                if display:
                    print(f"ç¬¬å››æ­¥æ˜¾ç¤ºå†…å®¹: {display[:100]}...")
                    summary_content += display
                    yield f"data: {json.dumps({'type': 'content', 'step': 4, 'content': display})}\n\n"
        except Exception as e:
            print(f"ç¬¬å››æ­¥AIè°ƒç”¨å‡ºé”™: {str(e)}")
            chunk_count = 0
            summary_content = ""
        
        print(f"æœ€ç»ˆæ±‡æ€»å®Œæˆï¼Œæ€»å…±æ”¶åˆ° {chunk_count} ä¸ªchunkï¼Œå†…å®¹é•¿åº¦: {len(summary_content)}")
        
        # å¦‚æœæ²¡æœ‰æ”¶åˆ°ä»»ä½•å†…å®¹ï¼Œç”Ÿæˆä¸€ä¸ªå¤‡ç”¨æ€»ç»“
        if not summary_content.strip() or chunk_count == 0:
            print("è­¦å‘Šï¼šAIæ²¡æœ‰è¿”å›ä»»ä½•å†…å®¹ï¼Œç”Ÿæˆå¤‡ç”¨æ€»ç»“...")
            
            # åŸºäºå‰é¢æ­¥éª¤çš„å†…å®¹ç”Ÿæˆä¸€ä¸ªç®€å•çš„æ€»ç»“
            fallback_summary = f"""## ğŸ“Š æœ€ç»ˆåˆ†ææ€»ç»“

### ğŸ¯ æ ¸å¿ƒå‘ç°
é€šè¿‡å‰é¢çš„åˆ†æï¼Œæˆ‘ä»¬å®Œæˆäº†ä»¥ä¸‹å·¥ä½œï¼š
- å¯¹å†…å®¹è¿›è¡Œäº†è¯¦ç»†çš„æ¦‚è¦åˆ†æ
- æœç´¢äº†ç›¸å…³çš„èƒŒæ™¯èµ„æ–™å’Œä¿¡æ¯
- è¿›è¡Œäº†æ·±å…¥çš„æ‰¹åˆ¤æ€§åˆ†æ

### ğŸ’ å…³é”®ä»·å€¼ç‚¹
- **ç†è®ºä»·å€¼**ï¼šå†…å®¹æä¾›äº†æœ‰ä»·å€¼çš„æŠ€æœ¯ä¿¡æ¯å’Œå®è·µæŒ‡å¯¼
- **å®è·µä»·å€¼**ï¼šå¯¹è¯»è€…å…·æœ‰å®é™…çš„å‚è€ƒå’Œåº”ç”¨ä»·å€¼
- **å¯å‘ä»·å€¼**ï¼šèƒ½å¤Ÿå¼•å‘è¯»è€…å¯¹ç›¸å…³æŠ€æœ¯çš„æ·±å…¥æ€è€ƒ

### ğŸ“‹ ç»¼åˆè¯„ä»·
- **ä¼˜ç‚¹**ï¼šå†…å®¹è¯¦å®ï¼Œæ­¥éª¤æ¸…æ™°ï¼Œå…·æœ‰è¾ƒå¼ºçš„å®ç”¨æ€§
- **ä»·å€¼**ï¼šä¸ºæŠ€æœ¯å­¦ä¹ å’Œå®è·µæä¾›äº†æœ‰æ•ˆçš„æŒ‡å¯¼
- **æ„ä¹‰**ï¼šæœ‰åŠ©äºè¯»è€…å¿«é€ŸæŒæ¡ç›¸å…³æŠ€æœ¯è¦ç‚¹

### ğŸ’¡ å®ç”¨å»ºè®®
- å»ºè®®è¯»è€…ç»“åˆå®é™…éœ€æ±‚ï¼Œé€‰æ‹©æ€§åœ°åº”ç”¨æ–‡ä¸­çš„æ–¹æ³•å’ŒæŠ€å·§
- åœ¨å®è·µè¿‡ç¨‹ä¸­æ³¨æ„ç¯å¢ƒé…ç½®å’Œç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜
- å¯ä»¥å‚è€ƒæ–‡ä¸­æä¾›çš„èµ„æºå’Œå·¥å…·æ¥æé«˜æ•ˆç‡

### ğŸ”® å»¶ä¼¸æ€è€ƒ
- å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢ç›¸å…³æŠ€æœ¯çš„æœ€æ–°å‘å±•å’Œåº”ç”¨åœºæ™¯
- æ€è€ƒå¦‚ä½•å°†æ‰€å­¦çŸ¥è¯†åº”ç”¨åˆ°å®é™…é¡¹ç›®ä¸­
- å…³æ³¨æŠ€æœ¯ç¤¾åŒºçš„æœ€ä½³å®è·µå’Œç»éªŒåˆ†äº«

*æ³¨ï¼šæœ¬æ€»ç»“åŸºäºå‰é¢çš„åˆ†ææ­¥éª¤ç”Ÿæˆã€‚*"""
            
            yield f"data: {json.dumps({'type': 'content', 'step': 4, 'content': fallback_summary})}\n\n"
        yield f"data: {json.dumps({'type': 'step_complete', 'step': 4})}\n\n"
        yield f"data: {json.dumps({'type': 'analysis_complete'})}\n\n"
        
        print("=" * 50)
        print("å…¨é¢åˆ†æå®Œæˆï¼")
        print("=" * 50)
        
    except Exception as e:
        print(f"å…¨é¢åˆ†æå‡ºé”™ï¼š{str(e)}")
        yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"

@comprehensive_analysis_bp.route('/chat', methods=['POST'])
def chat_with_ai():
    """ä¸AIèŠå¤© - å…¨é¢æ€»ç»“åŠŸèƒ½çš„ç»§ç»­å¯¹è¯"""
    try:
        data = request.get_json()
        session_id = data.get('session_id')
        user_message = data.get('message', '').strip()
        chat_history = data.get('chat_history', [])
        
        if not user_message:
            return jsonify({'success': False, 'message': 'æ¶ˆæ¯ä¸èƒ½ä¸ºç©º'})
        
        # æ„å»ºå¯¹è¯å†å²
        messages = []
        for msg in chat_history:
            messages.append({"role": msg['role'], "content": msg['content']})
        
        # æ·»åŠ ç”¨æˆ·æ–°æ¶ˆæ¯
        messages.append({"role": "user", "content": user_message})
        
        return Response(
            generate_chat_response(messages, session_id),
            mimetype='text/plain'
        )
        
    except Exception as e:
        return jsonify({'success': False, 'message': f'èŠå¤©å¤±è´¥ï¼š{str(e)}'})

def generate_chat_response(messages, session_id):
    """ç”ŸæˆèŠå¤©å“åº”"""
    try:
        yield f"data: {json.dumps({'type': 'session_id', 'session_id': session_id})}\n\n"
        
        # ç›´æ¥ä½¿ç”¨å·²æ„å»ºçš„æ¶ˆæ¯åˆ—è¡¨ï¼Œä¸æ·»åŠ é¢å¤–çš„ç©ºæ¶ˆæ¯
        model = ai_service.simple_model  # ä½¿ç”¨ç®€å•æ¨¡å‹è¿›è¡Œå¯¹è¯
        
        for chunk in ai_service._make_request(messages, model):
            # æå–æ€è€ƒå†…å®¹å’Œæ˜¾ç¤ºå†…å®¹
            thinking, display = ai_service.extract_thinking(chunk)
            
            if thinking:
                yield f"data: {json.dumps({'type': 'thinking', 'content': thinking})}\n\n"
            
            if display:
                yield f"data: {json.dumps({'type': 'content', 'content': display})}\n\n"
        
        yield f"data: {json.dumps({'type': 'done'})}\n\n"
        
    except Exception as e:
        yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"

def generate_comprehensive_analysis_json(content, session_id):
    """ç”Ÿæˆå…¨é¢åˆ†æçš„å››ä¸ªæ­¥éª¤ - JSONç‰ˆæœ¬ï¼ˆéæµå¼ï¼‰"""
    try:
        print(f"=== å…¨é¢åˆ†æè°ƒè¯•ï¼ˆJSONç‰ˆæœ¬ï¼‰===")
        print(f"ä¼šè¯ID: {session_id}")
        print(f"å†…å®¹é•¿åº¦: {len(content)}")
        print(f"å†…å®¹å‰200å­—ç¬¦: {content[:200]}")
        print(f"==================")
        
        # æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºç©ºæˆ–åŒ…å«çˆ¬è™«é”™è¯¯
        if not content.strip():
            raise Exception('å†…å®¹ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œåˆ†æ')
            
        if "é”™è¯¯ï¼šæ— æ³•æå–" in content:
            raise Exception('ç½‘é¡µå†…å®¹æå–å¤±è´¥ï¼Œè¯·æ£€æŸ¥é“¾æ¥æ˜¯å¦æœ‰æ•ˆ')
            
        if "æ²¡æœ‰æå–åˆ°ä»»ä½•å†…å®¹" in content:
            raise Exception('æ²¡æœ‰æå–åˆ°ä»»ä½•æœ‰æ•ˆå†…å®¹ï¼Œè¯·é‡æ–°å°è¯•')
        
        analysis_result = {
            'steps': []
        }
        
        # ç¬¬ä¸€æ­¥ï¼šæ–‡ç« æ¦‚è¦
        print("=" * 50)
        print("å¼€å§‹ç¬¬ä¸€æ­¥ï¼šæ–‡ç« æ¦‚è¦")
        print("=" * 50)
        
        overview_prompt = f"""æˆ‘æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹åˆ†æåŠ©æ‰‹ã€‚è¯·å¯¹ä»¥ä¸‹å†…å®¹è¿›è¡Œå…¨é¢çš„æ¦‚è¦åˆ†æã€‚

**å¾…åˆ†æå†…å®¹ï¼š**
{content}

**åˆ†æä»»åŠ¡ï¼š**
è¯·å¯¹ä¸Šè¿°å†…å®¹è¿›è¡Œå…¨é¢çš„æ¦‚è¦åˆ†æï¼ŒåŒ…æ‹¬ï¼š

## ğŸ“‹ æ–‡ç« æ¦‚è¦åˆ†æ

### ğŸ¯ ä¸»è¦ä¸»é¢˜
- æ˜ç¡®è¯†åˆ«æ–‡ç« çš„æ ¸å¿ƒä¸»é¢˜
- åˆ†æä¸»é¢˜çš„é‡è¦æ€§å’Œç›¸å…³æ€§

### ğŸ’¡ æ ¸å¿ƒè§‚ç‚¹
- æå–æ–‡ç« çš„æ ¸å¿ƒè§‚ç‚¹å’Œä¸»è¦è®ºè¿°
- åˆ†æè§‚ç‚¹çš„é€»è¾‘æ€§å’Œè¯´æœåŠ›

### ğŸ“Š ä¸»è¦è®ºè¿°å†…å®¹
- **ä¸»è¦è®ºç‚¹**ï¼šç³»ç»Ÿæ¢³ç†æ–‡ç« çš„ä¸»è¦è®ºç‚¹
- **è®ºè¿°é€»è¾‘**ï¼šåˆ†æè®ºè¿°çš„é€»è¾‘å…³ç³»å’Œç»“æ„
- **æ”¯æ’‘è¯æ®**ï¼šè¯†åˆ«ç”¨äºæ”¯æ’‘è®ºç‚¹çš„è¯æ®å’Œæ¡ˆä¾‹

### ğŸ” å…³é”®ä¿¡æ¯ç‚¹
- **é‡è¦äº‹å®å’Œæ•°æ®**ï¼šæå–é‡è¦çš„äº‹å®ã€æ•°æ®ã€ç»Ÿè®¡ä¿¡æ¯
- **å…³é”®æœ¯è¯­å’Œæ¦‚å¿µ**ï¼šè¯†åˆ«å¹¶è§£é‡Šå…³é”®æœ¯è¯­å’Œæ¦‚å¿µ
- **é‡è¦äººç‰©å’Œæœºæ„**ï¼šè¯†åˆ«æ–‡ä¸­æåˆ°çš„é‡è¦äººç‰©å’Œæœºæ„

### ğŸ“– æ–‡ç« ç»“æ„
- åˆ†ææ–‡ç« çš„ç»„ç»‡ç»“æ„å’Œæ®µè½å®‰æ’
- è¯„ä¼°ç»“æ„çš„åˆç†æ€§å’Œé€»è¾‘æ€§

è¯·ä½¿ç”¨æ¸…æ™°çš„markdownæ ¼å¼ï¼Œç¡®ä¿åˆ†æå…¨é¢è€Œæ·±å…¥ã€‚

<think>
æˆ‘éœ€è¦ä»”ç»†é˜…è¯»è¿™ç¯‡å†…å®¹ï¼Œä»å¤šä¸ªç»´åº¦è¿›è¡Œæ¦‚è¦åˆ†æï¼ŒåŒ…æ‹¬ä¸»é¢˜ã€è§‚ç‚¹ã€è®ºè¿°ã€ä¿¡æ¯ç‚¹å’Œç»“æ„ç­‰æ–¹é¢ã€‚
</think>"""
        
        print("æ­£åœ¨è°ƒç”¨AIè¿›è¡Œæ¦‚è¦åˆ†æ...")
        overview_content = ai_service.complex_chat_complete(overview_prompt)
        print(f"æ¦‚è¦åˆ†æå®Œæˆï¼Œå†…å®¹é•¿åº¦: {len(overview_content)}")
        print(f"æ¦‚è¦åˆ†æå†…å®¹é¢„è§ˆ: {overview_content[:300]}...")
        
        analysis_result['steps'].append({
            'step': 1,
            'name': 'æ–‡ç« æ¦‚è¦',
            'description': 'æå–æ–‡ç« å¤§æ„å’Œæ ¸å¿ƒä¿¡æ¯',
            'content': overview_content
        })
        
        # ç¬¬äºŒæ­¥ï¼šæœç´¢ç»“æœ
        print("=" * 50)
        print("å¼€å§‹ç¬¬äºŒæ­¥ï¼šæœç´¢ç»“æœ")
        print("=" * 50)
        
        # æå–æœç´¢å…³é”®è¯
        print("æ­£åœ¨æå–æœç´¢å…³é”®è¯...")
        keyword_prompt = f"""åŸºäºä»¥ä¸‹æ¦‚è¦åˆ†æç»“æœï¼š
{overview_content}

ä»¥åŠåŸæ–‡å†…å®¹ï¼š
{content}

è¯·æå–3-5ä¸ªæœ€é‡è¦çš„æœç´¢å…³é”®è¯ï¼Œç”¨äºæœç´¢ç›¸å…³èµ„æ–™å’Œä¿¡æ¯ã€‚
å…³é”®è¯åº”è¯¥æ˜¯ï¼š
1. æ–‡ç« çš„æ ¸å¿ƒä¸»é¢˜
2. é‡è¦çš„æ¦‚å¿µæˆ–æœ¯è¯­
3. å¯èƒ½éœ€è¦è¿›ä¸€æ­¥äº†è§£çš„è¯é¢˜
4. ç›¸å…³çš„äººç‰©ã€æœºæ„æˆ–äº‹ä»¶

è¯·ç›´æ¥è¾“å‡ºå…³é”®è¯ï¼Œæ¯è¡Œä¸€ä¸ªï¼Œä¸è¦å…¶ä»–æ ¼å¼å’Œè§£é‡Šã€‚"""
        
        keywords_text = ai_service.simple_chat_complete(keyword_prompt)
        keywords = [kw.strip() for kw in keywords_text.split('\n') if kw.strip() and not kw.strip().startswith('#') and not kw.strip().startswith('*')]
        keywords = keywords[:5]  # é™åˆ¶æœ€å¤š5ä¸ªå…³é”®è¯
        
        print(f"æå–åˆ°çš„å…³é”®è¯: {keywords}")
        
        search_results_content = "## ğŸ” æœç´¢ç»“æœæ±‡æ€»\n\n"
        search_results_content += f"åŸºäºæ–‡ç« å†…å®¹ï¼Œæˆ‘ä»¬æå–äº†ä»¥ä¸‹å…³é”®è¯è¿›è¡Œæœç´¢ï¼š`{'`ã€`'.join(keywords)}`\n\n"
        
        search_results_data = []
        
        for i, keyword in enumerate(keywords, 1):
            print(f"æ­£åœ¨æœç´¢å…³é”®è¯ {i}/{len(keywords)}: {keyword}")
            search_results_content += f"### {i}. å…³é”®è¯ï¼š{keyword}\n\n"
            
            search_data = search_information(keyword)
            if 'error' not in search_data and 'results' in search_data and search_data['results']:
                results = search_data['results'][:3]  # å–å‰3ä¸ªç»“æœ
                print(f"  æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç»“æœ")
                
                for j, result in enumerate(results, 1):
                    title = result.get('title', 'æœªçŸ¥æ ‡é¢˜')
                    snippet = result.get('snippet', result.get('description', 'æ— æè¿°'))
                    url = result.get('url', '#')
                    
                    search_results_content += f"**ç»“æœ {j}**ï¼š[{title}]({url})\n"
                    search_results_content += f"ğŸ“„ {snippet}\n\n"
                    
                    # ä¿å­˜æœç´¢ç»“æœæ•°æ®ä¾›åç»­åˆ†æä½¿ç”¨
                    search_results_data.append({
                        'keyword': keyword,
                        'title': title,
                        'snippet': snippet,
                        'url': url
                    })
            else:
                print(f"  æœç´¢ '{keyword}' æ—¶å‡ºç°é”™è¯¯æˆ–æ— ç»“æœ")
                search_results_content += f"âŒ æœç´¢ '{keyword}' æ—¶å‡ºç°é”™è¯¯æˆ–æ— ç»“æœ\n\n"
        
        print(f"æœç´¢ç»“æœæ±‡æ€»å®Œæˆï¼Œå…±è·å¾— {len(search_results_data)} æ¡æœ‰æ•ˆç»“æœ")
        
        analysis_result['steps'].append({
            'step': 2,
            'name': 'æœç´¢ç»“æœ',
            'description': 'æœç´¢ç›¸å…³èµ„æ–™å’Œä¿¡æ¯',
            'content': search_results_content
        })
        
        # ç¬¬ä¸‰æ­¥ï¼šæ·±åº¦åˆ†æ
        print("=" * 50)
        print("å¼€å§‹ç¬¬ä¸‰æ­¥ï¼šæ·±åº¦åˆ†æ")
        print("=" * 50)
        
        # æ„å»ºæœç´¢ç»“æœä¸Šä¸‹æ–‡
        search_context = ""
        if search_results_data:
            search_context = "ç›¸å…³æœç´¢ç»“æœï¼š\n"
            for result in search_results_data:
                search_context += f"- å…³é”®è¯'{result['keyword']}'ï¼š{result['title']} - {result['snippet']}\n"
        
        # é™åˆ¶å†…å®¹é•¿åº¦ï¼Œé¿å…promptè¿‡é•¿
        overview_summary = overview_content[:800] + "..." if len(overview_content) > 800 else overview_content
        search_summary = search_context[:500] + "..." if len(search_context) > 500 else search_context
        content_summary = content[:1000] + "..." if len(content) > 1000 else content
        
        deep_analysis_prompt = f"""åŸºäºå‰é¢çš„æ¦‚è¦åˆ†æï¼š
{overview_summary}

ä»¥åŠæœç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯ï¼š
{search_summary}

è¯·å¯¹ä»¥ä¸‹å†…å®¹è¿›è¡Œæ·±åº¦åˆ†æï¼š
{content_summary}

è¯·è¿›è¡Œç®€æ´è€Œæ·±å…¥çš„åˆ†æï¼š

## ğŸ”¬ æ·±åº¦åˆ†æ

### ğŸ’ å†…å®¹ä»·å€¼è¯„ä¼°
- åˆ†æå†…å®¹çš„å®ç”¨ä»·å€¼å’ŒæŒ‡å¯¼æ„ä¹‰
- è¯„ä¼°å†…å®¹çš„åˆ›æ–°æ€§å’Œç‹¬ç‰¹ä¹‹å¤„

### ğŸ­ è§‚ç‚¹æ·±åº¦è§£è¯»  
- åˆ†æè§‚ç‚¹çš„æ·±åº¦å’Œå¤æ‚æ€§
- è¯„ä¼°è®ºè¯çš„ä¸¥å¯†æ€§å’Œè¯´æœåŠ›

### ğŸ”— é€»è¾‘å…³ç³»åˆ†æ
- åˆ†æè®ºè¯çš„é€»è¾‘ç»“æ„å’Œæ¨ç†è¿‡ç¨‹
- è¯†åˆ«å†…å®¹ä¸­çš„å› æœå…³ç³»

### ğŸŒ èƒŒæ™¯å’Œä¸Šä¸‹æ–‡
- åˆ†æå†…å®¹çš„å†å²èƒŒæ™¯å’Œå‘å±•è„‰ç»œ
- è¯„ä¼°å†…å®¹åœ¨å½“å‰æ—¶ä»£çš„æ„ä¹‰å’Œä»·å€¼

### ğŸ¤” æ‰¹åˆ¤æ€§æ€è€ƒ
- å¯¹å†…å®¹è¿›è¡Œæ‰¹åˆ¤æ€§è´¨ç–‘å’Œåæ€
- æå‡ºå¯èƒ½çš„æ”¹è¿›å»ºè®®

è¯·ä½¿ç”¨æ¸…æ™°çš„markdownæ ¼å¼ï¼Œä¿æŒåˆ†æç®€æ´æ˜äº†ã€‚

<think>
æˆ‘éœ€è¦å¯¹è¿™ä¸ªå†…å®¹è¿›è¡Œæ·±å…¥çš„æ‰¹åˆ¤æ€§åˆ†æï¼Œä»å¤šä¸ªç»´åº¦è¯„ä¼°å…¶ä»·å€¼ã€è§‚ç‚¹ã€é€»è¾‘å’Œæ„ä¹‰ã€‚
</think>"""
        
        print("æ­£åœ¨è¿›è¡Œæ·±åº¦åˆ†æ...")
        print(f"æ·±åº¦åˆ†æprompté•¿åº¦: {len(deep_analysis_prompt)}")
        try:
            deep_analysis_content = ai_service.complex_chat_complete(deep_analysis_prompt)
            print(f"æ·±åº¦åˆ†æå®Œæˆï¼Œå†…å®¹é•¿åº¦: {len(deep_analysis_content)}")
            print(f"æ·±åº¦åˆ†æå†…å®¹é¢„è§ˆ: {deep_analysis_content[:300]}...")
        except Exception as e:
            print(f"ç¬¬ä¸‰æ­¥AIè°ƒç”¨å‡ºé”™: {str(e)}")
            import traceback
            traceback.print_exc()
            deep_analysis_content = ""
        
        analysis_result['steps'].append({
            'step': 3,
            'name': 'æ·±åº¦åˆ†æ',
            'description': 'æ·±å…¥åˆ†æå†…å®¹ä»·å€¼å’Œæ„ä¹‰',
            'content': deep_analysis_content
        })
        
        # ç¬¬å››æ­¥ï¼šç»“æœæ±‡æ€»
        print("=" * 50)
        print("å¼€å§‹ç¬¬å››æ­¥ï¼šç»“æœæ±‡æ€»")
        print("=" * 50)
        
        # é™åˆ¶å‰é¢æ­¥éª¤å†…å®¹çš„é•¿åº¦ï¼Œé¿å…promptè¿‡é•¿
        overview_summary = overview_content[:600] + "..." if len(overview_content) > 600 else overview_content
        search_summary = search_results_content[:400] + "..." if len(search_results_content) > 400 else search_results_content
        deep_summary = deep_analysis_content[:600] + "..." if len(deep_analysis_content) > 600 else deep_analysis_content
        
        summary_prompt = f"""åŸºäºå‰é¢çš„åˆ†æï¼š

**æ¦‚è¦åˆ†ææ‘˜è¦ï¼š**
{overview_summary}

**æœç´¢ç»“æœæ‘˜è¦ï¼š**
{search_summary}

**æ·±åº¦åˆ†ææ‘˜è¦ï¼š**
{deep_summary}

è¯·ç”Ÿæˆæœ€ç»ˆçš„ç»¼åˆæ€»ç»“ï¼š

## ğŸ“Š æœ€ç»ˆåˆ†ææ€»ç»“

### ğŸ¯ æ ¸å¿ƒå‘ç°
- æ€»ç»“åˆ†æè¿‡ç¨‹ä¸­æœ€é‡è¦çš„å‘ç°å’Œæ´å¯Ÿ
- æç‚¼å‡ºæœ€æ ¸å¿ƒçš„è§‚ç‚¹å’Œè®ºè¿°

### ğŸ’ å…³é”®ä»·å€¼ç‚¹
- **ç†è®ºä»·å€¼**ï¼šæ€»ç»“å†…å®¹çš„ç†è®ºè´¡çŒ®å’Œå­¦æœ¯ä»·å€¼
- **å®è·µä»·å€¼**ï¼šåˆ†æå†…å®¹çš„å®é™…åº”ç”¨ä»·å€¼å’ŒæŒ‡å¯¼æ„ä¹‰
- **å¯å‘ä»·å€¼**ï¼šæç‚¼å¯¹è¯»è€…æœ€æœ‰å¯å‘æ„ä¹‰çš„å†…å®¹

### ğŸ“‹ ç»¼åˆè¯„ä»·
- **ä¼˜ç‚¹åˆ†æ**ï¼šæ€»ç»“å†…å®¹çš„ä¸»è¦ä¼˜ç‚¹å’Œäº®ç‚¹
- **ä¸è¶³ä¹‹å¤„**ï¼šæŒ‡å‡ºå†…å®¹å¯èƒ½å­˜åœ¨çš„ä¸è¶³æˆ–å±€é™
- **æ•´ä½“è¯„ä¼°**ï¼šç»™å‡ºå¯¹å†…å®¹çš„æ•´ä½“è¯„ä»·å’Œå®šä½

### ğŸ’¡ å®ç”¨å»ºè®®
- **é˜…è¯»å»ºè®®**ï¼šä¸ºè¯»è€…æä¾›é˜…è¯»å’Œç†è§£çš„å»ºè®®
- **åº”ç”¨å»ºè®®**ï¼šæä¾›å¦‚ä½•åº”ç”¨å†…å®¹çš„å®ç”¨å»ºè®®
- **å­¦ä¹ å»ºè®®**ï¼šä¸ºè¿›ä¸€æ­¥å­¦ä¹ æä¾›æ–¹å‘æ€§å»ºè®®

### ğŸ”® å»¶ä¼¸æ€è€ƒ
- **æ·±å…¥é—®é¢˜**ï¼šæå‡ºå€¼å¾—è¿›ä¸€æ­¥æ€è€ƒå’Œæ¢ç´¢çš„é—®é¢˜
- **ç›¸å…³é¢†åŸŸ**ï¼šæ¨èç›¸å…³çš„ç ”ç©¶é¢†åŸŸæˆ–è¯é¢˜
- **æœªæ¥å‘å±•**ï¼šåˆ†æç›¸å…³é¢†åŸŸçš„æœªæ¥å‘å±•è¶‹åŠ¿

è¯·æä¾›ä¸€ä¸ªå…¨é¢ã€æ·±å…¥ã€æœ‰ä»·å€¼çš„æœ€ç»ˆæ€»ç»“ï¼Œä½¿ç”¨æ¸…æ™°çš„markdownæ ¼å¼ã€‚

<think>
æˆ‘éœ€è¦å°†å‰é¢çš„æ‰€æœ‰åˆ†ææ•´åˆèµ·æ¥ï¼Œå½¢æˆä¸€ä¸ªå®Œæ•´ã€æœ‰æ¡ç†ã€æœ‰ä»·å€¼çš„æœ€ç»ˆæŠ¥å‘Šã€‚
</think>"""
        
        print("æ­£åœ¨ç”Ÿæˆæœ€ç»ˆæ±‡æ€»...")
        print(f"æœ€ç»ˆæ±‡æ€»prompté•¿åº¦: {len(summary_prompt)}")
        try:
            summary_content = ai_service.complex_chat_complete(summary_prompt)
            print(f"æœ€ç»ˆæ±‡æ€»å®Œæˆï¼Œå†…å®¹é•¿åº¦: {len(summary_content)}")
            print(f"æœ€ç»ˆæ±‡æ€»å†…å®¹é¢„è§ˆ: {summary_content[:300]}...")
        except Exception as e:
            print(f"ç¬¬å››æ­¥AIè°ƒç”¨å‡ºé”™: {str(e)}")
            import traceback
            traceback.print_exc()
            summary_content = ""
        
        # å¦‚æœæ²¡æœ‰æ”¶åˆ°ä»»ä½•å†…å®¹ï¼Œç”Ÿæˆä¸€ä¸ªå¤‡ç”¨æ€»ç»“
        if not summary_content.strip():
            print("è­¦å‘Šï¼šAIæ²¡æœ‰è¿”å›ä»»ä½•å†…å®¹ï¼Œç”Ÿæˆå¤‡ç”¨æ€»ç»“...")
            summary_content = f"""## ğŸ“Š æœ€ç»ˆåˆ†ææ€»ç»“

### ğŸ¯ æ ¸å¿ƒå‘ç°
é€šè¿‡å‰é¢çš„åˆ†æï¼Œæˆ‘ä»¬å®Œæˆäº†ä»¥ä¸‹å·¥ä½œï¼š
- å¯¹å†…å®¹è¿›è¡Œäº†è¯¦ç»†çš„æ¦‚è¦åˆ†æ
- æœç´¢äº†ç›¸å…³çš„èƒŒæ™¯èµ„æ–™å’Œä¿¡æ¯
- è¿›è¡Œäº†æ·±å…¥çš„æ‰¹åˆ¤æ€§åˆ†æ

### ğŸ’ å…³é”®ä»·å€¼ç‚¹
- **ç†è®ºä»·å€¼**ï¼šå†…å®¹æä¾›äº†æœ‰ä»·å€¼çš„æŠ€æœ¯ä¿¡æ¯å’Œå®è·µæŒ‡å¯¼
- **å®è·µä»·å€¼**ï¼šå¯¹è¯»è€…å…·æœ‰å®é™…çš„å‚è€ƒå’Œåº”ç”¨ä»·å€¼
- **å¯å‘ä»·å€¼**ï¼šèƒ½å¤Ÿå¼•å‘è¯»è€…å¯¹ç›¸å…³æŠ€æœ¯çš„æ·±å…¥æ€è€ƒ

### ğŸ“‹ ç»¼åˆè¯„ä»·
- **ä¼˜ç‚¹**ï¼šå†…å®¹è¯¦å®ï¼Œæ­¥éª¤æ¸…æ™°ï¼Œå…·æœ‰è¾ƒå¼ºçš„å®ç”¨æ€§
- **ä»·å€¼**ï¼šä¸ºæŠ€æœ¯å­¦ä¹ å’Œå®è·µæä¾›äº†æœ‰æ•ˆçš„æŒ‡å¯¼
- **æ„ä¹‰**ï¼šæœ‰åŠ©äºè¯»è€…å¿«é€ŸæŒæ¡ç›¸å…³æŠ€æœ¯è¦ç‚¹

### ğŸ’¡ å®ç”¨å»ºè®®
- å»ºè®®è¯»è€…ç»“åˆå®é™…éœ€æ±‚ï¼Œé€‰æ‹©æ€§åœ°åº”ç”¨æ–‡ä¸­çš„æ–¹æ³•å’ŒæŠ€å·§
- åœ¨å®è·µè¿‡ç¨‹ä¸­æ³¨æ„ç¯å¢ƒé…ç½®å’Œç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜
- å¯ä»¥å‚è€ƒæ–‡ä¸­æä¾›çš„èµ„æºå’Œå·¥å…·æ¥æé«˜æ•ˆç‡

### ğŸ”® å»¶ä¼¸æ€è€ƒ
- å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢ç›¸å…³æŠ€æœ¯çš„æœ€æ–°å‘å±•å’Œåº”ç”¨åœºæ™¯
- æ€è€ƒå¦‚ä½•å°†æ‰€å­¦çŸ¥è¯†åº”ç”¨åˆ°å®é™…é¡¹ç›®ä¸­
- å…³æ³¨æŠ€æœ¯ç¤¾åŒºçš„æœ€ä½³å®è·µå’Œç»éªŒåˆ†äº«

*æ³¨ï¼šæœ¬æ€»ç»“åŸºäºå‰é¢çš„åˆ†ææ­¥éª¤ç”Ÿæˆã€‚*"""
            print(f"å¤‡ç”¨æ€»ç»“ç”Ÿæˆå®Œæˆï¼Œå†…å®¹é•¿åº¦: {len(summary_content)}")
        
        analysis_result['steps'].append({
            'step': 4,
            'name': 'ç»“æœæ±‡æ€»',
            'description': 'æ±‡æ€»åˆ†æç»“æœå’Œå…³é”®å‘ç°',
            'content': summary_content
        })
        
        print("=" * 50)
        print("å…¨é¢åˆ†æå®Œæˆï¼")
        print(f"æ€»å…±ç”Ÿæˆäº† {len(analysis_result['steps'])} ä¸ªåˆ†ææ­¥éª¤")
        for step in analysis_result['steps']:
            print(f"  æ­¥éª¤ {step['step']}: {step['name']} - å†…å®¹é•¿åº¦: {len(step['content'])}")
        print("=" * 50)
        
        return analysis_result
        
    except Exception as e:
        print(f"å…¨é¢åˆ†æå‡ºé”™ï¼š{str(e)}")
        print("é”™è¯¯è¯¦æƒ…ï¼š", e)
        raise e
