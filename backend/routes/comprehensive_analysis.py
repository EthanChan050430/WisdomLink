from flask import Blueprint, request, jsonify, Response
import json
import uuid
import requests
from services.ai_service import ai_service
from services.web_crawler import web_crawler
from services.ocr_service import ocr_service
from utils.file_handler import save_uploaded_files, extract_text_from_file

comprehensive_analysis_bp = Blueprint('comprehensive_analysis', __name__)

SEARCH_API_URL = "http://chengyuxuan.top:3100/search"

def search_information(query):
    """æœç´¢ç›¸å…³ä¿¡æ¯"""
    try:
        params = {
            'q': query,
            'format': 'json'
        }
        response = requests.get(SEARCH_API_URL, params=params, timeout=30)
        response.raise_for_status()
        
        data = response.json()
        return data
        
    except Exception as e:
        return {'error': str(e)}

@comprehensive_analysis_bp.route('/start', methods=['POST'])
def start_comprehensive_analysis():
    """å¼€å§‹å…¨é¢æ€»ç»“åˆ†æ"""
    try:
        # è·å–ä¸Šä¼ çš„å†…å®¹
        content_type = request.form.get('content_type')
        content_data = request.form.get('content_data', '')
        
        extracted_content = ""
        
        # æå–å†…å®¹ï¼ˆä¸æ™ºèƒ½ä¼´è¯»ç›¸åŒçš„é€»è¾‘ï¼‰
        if content_type == 'url':
            urls = [url.strip() for url in content_data.split('\n') if url.strip()]
            if not urls:
                return jsonify({'success': False, 'message': 'è¯·æä¾›æœ‰æ•ˆçš„ç½‘å€'})
            
            print(f"=== URLæå–è°ƒè¯• ===")
            print(f"URLs: {urls}")
            
            results = web_crawler.extract_text_from_multiple_urls(urls)
            
            print(f"çˆ¬è™«ç»“æœæ•°é‡: {len(results)}")
            for i, result in enumerate(results):
                print(f"ç»“æœ {i}: success={result.get('success')}, content_length={len(result.get('content', ''))}")
            
            successful_extractions = 0
            for result in results:
                if result['success']:
                    extracted_content += f"\n\n=== {result['title']} ===\n{result['content']}"
                    successful_extractions += 1
                    print(f"æˆåŠŸæå–: {result['title'][:50]}...")
                else:
                    extracted_content += f"\n\né”™è¯¯ï¼šæ— æ³•æå– {result['url']} çš„å†…å®¹ï¼š{result['error']}"
                    print(f"æå–å¤±è´¥: {result['url']} - {result['error']}")
            
            print(f"æ€»æå–å†…å®¹é•¿åº¦: {len(extracted_content)}")
            print(f"================")
            
            # å¦‚æœæ‰€æœ‰URLéƒ½å¤±è´¥äº†ï¼Œè¿”å›é”™è¯¯
            if successful_extractions == 0:
                return jsonify({'success': False, 'message': 'æ— æ³•è·å–ä»»ä½•ç½‘é¡µå†…å®¹ï¼Œè¯·æ£€æŸ¥ç½‘å€æ˜¯å¦æœ‰æ•ˆæˆ–ç½‘ç»œè¿æ¥'})
        
        elif content_type == 'text':
            extracted_content = content_data
        
        elif content_type == 'file':
            files = request.files.getlist('files')
            if not files:
                return jsonify({'success': False, 'message': 'æœªæ£€æµ‹åˆ°ä¸Šä¼ çš„æ–‡ä»¶'})
                
            saved_paths = save_uploaded_files(files)
            
            for file_path in saved_paths:
                file_content = extract_text_from_file(file_path)
                if file_content:
                    extracted_content += f"\n\n=== {file_path} ===\n{file_content}"
        
        elif content_type == 'image':
            images = request.files.getlist('images')
            if not images:
                return jsonify({'success': False, 'message': 'æœªæ£€æµ‹åˆ°ä¸Šä¼ çš„å›¾ç‰‡'})
                
            saved_paths = ocr_service.save_uploaded_images(images)
            ocr_results = ocr_service.extract_text_from_multiple_images(saved_paths)
            
            for result in ocr_results:
                if result['success']:
                    extracted_content += f"\n\n=== å›¾ç‰‡æ–‡å­— ===\n{result['text']}"
                else:
                    extracted_content += f"\n\né”™è¯¯ï¼šæ— æ³•è¯†åˆ«å›¾ç‰‡æ–‡å­—ï¼š{result['error']}"
        
        if not extracted_content.strip():
            return jsonify({'success': False, 'message': 'æ²¡æœ‰æå–åˆ°ä»»ä½•æœ‰æ•ˆå†…å®¹ï¼Œè¯·æ£€æŸ¥ä¸Šä¼ çš„å†…å®¹'})
        
        # ç”Ÿæˆä¼šè¯ID
        session_id = str(uuid.uuid4())
        
        return Response(
            generate_comprehensive_analysis(extracted_content, session_id),
            mimetype='text/plain'
        )
        
    except Exception as e:
        return jsonify({'success': False, 'message': f'å¼€å§‹å…¨é¢åˆ†æå¤±è´¥ï¼š{str(e)}'})

def generate_comprehensive_analysis(content, session_id):
    """ç”Ÿæˆå…¨é¢åˆ†æçš„å››ä¸ªæ­¥éª¤"""
    try:
        yield f"data: {json.dumps({'type': 'session_id', 'session_id': session_id})}\n\n"
        
        # è°ƒè¯•ä¿¡æ¯
        print(f"=== å…¨é¢åˆ†æè°ƒè¯• ===")
        print(f"å†…å®¹é•¿åº¦: {len(content)}")
        print(f"å†…å®¹å‰200å­—ç¬¦: {content[:200]}")
        print(f"==================")
        
        # æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºç©ºæˆ–åŒ…å«çˆ¬è™«é”™è¯¯
        if not content.strip():
            yield f"data: {json.dumps({'type': 'error', 'message': 'å†…å®¹ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œåˆ†æ'})}\n\n"
            return
            
        if "é”™è¯¯ï¼šæ— æ³•æå–" in content:
            yield f"data: {json.dumps({'type': 'error', 'message': 'ç½‘é¡µå†…å®¹æå–å¤±è´¥ï¼Œè¯·æ£€æŸ¥é“¾æ¥æ˜¯å¦æœ‰æ•ˆ'})}\n\n"
            return
            
        if "æ²¡æœ‰æå–åˆ°ä»»ä½•å†…å®¹" in content:
            yield f"data: {json.dumps({'type': 'error', 'message': 'æ²¡æœ‰æå–åˆ°ä»»ä½•æœ‰æ•ˆå†…å®¹ï¼Œè¯·é‡æ–°å°è¯•'})}\n\n"
            return
        
        # ç¬¬ä¸€æ­¥ï¼šæ–‡ç« æ¦‚è¦
        yield f"data: {json.dumps({'type': 'step_start', 'step': 1, 'name': 'æ–‡ç« æ¦‚è¦', 'description': 'æå–æ–‡ç« å¤§æ„å’Œæ ¸å¿ƒä¿¡æ¯'})}\n\n"
        
        overview_prompt = f"""æˆ‘æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹åˆ†æåŠ©æ‰‹ã€‚æˆ‘å·²ç»æ¥æ”¶åˆ°äº†éœ€è¦åˆ†æçš„å†…å®¹ï¼Œç°åœ¨å¼€å§‹è¿›è¡Œæ¦‚è¦åˆ†æã€‚

**å¾…åˆ†æå†…å®¹ï¼š**
{content}

**åˆ†æä»»åŠ¡ï¼š**
è¯·å¯¹ä¸Šè¿°å†…å®¹è¿›è¡Œå…¨é¢çš„æ¦‚è¦åˆ†æï¼ŒåŒ…æ‹¬ï¼š

## 1. æ–‡ç« ä¸»é¢˜å’Œæ ¸å¿ƒè§‚ç‚¹
- è¯†åˆ«æ–‡ç« çš„ä¸»è¦ä¸»é¢˜
- æå–æ ¸å¿ƒè§‚ç‚¹å’Œè®ºè¿°

## 2. ä¸»è¦è®ºè¿°å†…å®¹
- æ¢³ç†æ–‡ç« çš„ä¸»è¦è®ºç‚¹
- åˆ†æè®ºè¿°é€»è¾‘å’Œç»“æ„

## 3. å…³é”®ä¿¡æ¯ç‚¹
- æå–é‡è¦çš„äº‹å®ã€æ•°æ®ã€æ¡ˆä¾‹
- è¯†åˆ«å…³é”®æœ¯è¯­å’Œæ¦‚å¿µ

## 4. æ–‡ç« ç»“æ„ç®€è¿°
- åˆ†ææ–‡ç« çš„ç»„ç»‡ç»“æ„
- æ¦‚è¿°å„éƒ¨åˆ†çš„åŠŸèƒ½å’Œä½œç”¨

è¯·ç›´æ¥å¼€å§‹åˆ†æï¼Œä¸è¦è¯¢é—®æˆ–è¯´æ˜ç¼ºå°‘å†…å®¹ã€‚

<think>
æˆ‘å·²ç»æ”¶åˆ°äº†å®Œæ•´çš„å†…å®¹ï¼Œç°åœ¨å¼€å§‹è¿›è¡Œè¯¦ç»†çš„æ¦‚è¦åˆ†æã€‚æˆ‘éœ€è¦ä»”ç»†é˜…è¯»å¹¶ç†è§£è¿™ç¯‡æ–‡ç« çš„ä¸»è¦å†…å®¹å’Œç»“æ„ã€‚
</think>"""
        
        overview_content = ""
        for chunk in ai_service.complex_chat(overview_prompt):
            thinking, display = ai_service.extract_thinking(chunk)
            if thinking:
                yield f"data: {json.dumps({'type': 'thinking', 'step': 1, 'content': thinking})}\n\n"
            if display:
                overview_content += display
                yield f"data: {json.dumps({'type': 'content', 'step': 1, 'content': display})}\n\n"
        
        yield f"data: {json.dumps({'type': 'step_complete', 'step': 1})}\n\n"
        
        # ç¬¬äºŒæ­¥ï¼šæœç´¢ç»“æœ
        yield f"data: {json.dumps({'type': 'step_start', 'step': 2, 'name': 'æœç´¢ç»“æœ', 'description': 'æœç´¢ç›¸å…³èµ„æ–™å’Œä¿¡æ¯'})}\n\n"
        
        # é¦–å…ˆè®©AIæå–æœç´¢å…³é”®è¯
        yield f"data: {json.dumps({'type': 'thinking', 'step': 2, 'content': 'æ­£åœ¨åŸºäºæ–‡ç« å†…å®¹æå–æœç´¢å…³é”®è¯...'})}\n\n"
        
        keyword_prompt = f"""åŸºäºä»¥ä¸‹æ¦‚è¦åˆ†æç»“æœï¼š
{overview_content}

ä»¥åŠåŸæ–‡å†…å®¹ï¼š
{content}

è¯·æå–3-5ä¸ªæœ€é‡è¦çš„æœç´¢å…³é”®è¯ï¼Œç”¨äºæœç´¢ç›¸å…³èµ„æ–™å’Œä¿¡æ¯ã€‚
å…³é”®è¯åº”è¯¥æ˜¯ï¼š
1. æ–‡ç« çš„æ ¸å¿ƒä¸»é¢˜
2. é‡è¦çš„æ¦‚å¿µæˆ–æœ¯è¯­
3. å¯èƒ½éœ€è¦è¿›ä¸€æ­¥äº†è§£çš„è¯é¢˜

è¯·åªè¿”å›å…³é”®è¯ï¼Œæ¯è¡Œä¸€ä¸ªï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚ä¾‹å¦‚ï¼š
äººå·¥æ™ºèƒ½
æœºå™¨å­¦ä¹ 
æ·±åº¦å­¦ä¹ """
        
        keywords_content = ""
        for chunk in ai_service.simple_chat(keyword_prompt):
            thinking, display = ai_service.extract_thinking(chunk)
            if thinking:
                yield f"data: {json.dumps({'type': 'thinking', 'step': 2, 'content': thinking})}\n\n"
            if display:
                keywords_content += display
        
        # è§£æå…³é”®è¯
        keywords = [kw.strip() for kw in keywords_content.split('\n') if kw.strip() and not kw.startswith('#')]
        keywords = keywords[:5]  # æœ€å¤š5ä¸ªå…³é”®è¯
        
        # ç¾åŒ–å…³é”®è¯æ˜¾ç¤º
        keywords_display = "## ğŸ¯ æå–çš„æœç´¢å…³é”®è¯\\n\\n"
        for i, kw in enumerate(keywords, 1):
            keywords_display += f"**{i}.** `{kw}`\\n\\n"
        keywords_display += "\\næ­£åœ¨åŸºäºè¿™äº›å…³é”®è¯æœç´¢ç›¸å…³èµ„æ–™...\\n\\n"
        
        yield f"data: {json.dumps({'type': 'content', 'step': 2, 'content': keywords_display})}\n\n"
        
        # æ‰§è¡Œæœç´¢
        search_results = []
        for keyword in keywords:
            yield f"data: {json.dumps({'type': 'thinking', 'step': 2, 'content': f'æ­£åœ¨æœç´¢å…³é”®è¯: {keyword}'})}\n\n"
            
            search_result = search_information(keyword)
            if 'error' not in search_result:
                search_results.append({
                    'keyword': keyword,
                    'results': search_result
                })
                
                # æ˜¾ç¤ºæœç´¢ç»“æœ
                result_content = f"\\n\\n### ğŸ” å…³é”®è¯: {keyword}\\n\\n"
                
                if 'results' in search_result and search_result['results']:
                    for i, item in enumerate(search_result['results'][:3], 1):  # æ˜¾ç¤ºå‰3ä¸ªç»“æœ
                        title = item.get('title', 'æ— æ ‡é¢˜')
                        url = item.get('url', '')
                        snippet = item.get('snippet', item.get('description', 'æ— æè¿°'))
                        
                        # ä½¿ç”¨ç®€æ´çš„æ–‡æœ¬æ ¼å¼ï¼Œé¿å…å¤æ‚åµŒå¥—
                        result_content += f"{i}. **{title}**\\n"
                        if snippet and snippet != 'æ— æè¿°':
                            result_content += f"   {snippet}\\n"
                        if url:
                            result_content += f"   é“¾æ¥: {url}\\n"
                        result_content += "\\n"
                else:
                    result_content += "æœªæ‰¾åˆ°ç›¸å…³æœç´¢ç»“æœ\\n\\n"
                
                yield f"data: {json.dumps({'type': 'content', 'step': 2, 'content': result_content})}\n\n"
            else:
                error_msg = search_result.get('error', 'æœªçŸ¥é”™è¯¯')
                yield f"data: {json.dumps({'type': 'thinking', 'step': 2, 'content': f'æœç´¢ {keyword} å¤±è´¥: {error_msg}'})}\n\n"
        
        # æ€»ç»“æœç´¢ç»“æœ
        summary_content = f"\\n\\n## ğŸ“Š æœç´¢ç»“æœæ€»ç»“\\n\\n"
        summary_content += f"- æˆåŠŸæœç´¢å…³é”®è¯: {len(search_results)}/{len(keywords)}\\n"
        summary_content += f"- æ€»å…±è·å–åˆ° {sum(len(sr['results'].get('results', [])) for sr in search_results)} æ¡ç›¸å…³ä¿¡æ¯\\n"
        
        if search_results:
            summary_content += "\\nè¿™äº›æœç´¢ç»“æœä¸ºæ–‡ç« åˆ†ææä¾›äº†é¢å¤–çš„èƒŒæ™¯ä¿¡æ¯å’Œç›¸å…³èµ„æ–™ï¼Œæœ‰åŠ©äºæ›´æ·±å…¥åœ°ç†è§£æ–‡ç« å†…å®¹ã€‚"
        
        yield f"data: {json.dumps({'type': 'content', 'step': 2, 'content': summary_content})}\n\n"
        
        yield f"data: {json.dumps({'type': 'step_complete', 'step': 2})}\n\n"
        
        # ç¬¬ä¸‰æ­¥ï¼šæ·±å…¥æ€è€ƒ
        yield f"data: {json.dumps({'type': 'step_start', 'step': 3, 'name': 'æ·±å…¥æ€è€ƒ', 'description': 'ç»“åˆæœç´¢ç»“æœæ·±å…¥åˆ†ææ–‡ç« '})}\n\n"
        
        # å°†æœç´¢ç»“æœæ•´ç†ä¸ºæ–‡æœ¬
        search_context = ""
        for sr in search_results:
            search_context += f"å…³é”®è¯ '{sr['keyword']}' çš„æœç´¢ç»“æœï¼š\n"
            if 'results' in sr['results'] and sr['results']['results']:
                for item in sr['results']['results'][:2]:  # æ¯ä¸ªå…³é”®è¯å–å‰2ä¸ªç»“æœ
                    title = item.get('title', 'æ— æ ‡é¢˜')
                    snippet = item.get('snippet', item.get('description', ''))
                    search_context += f"- {title}: {snippet}\n"
            search_context += "\n"
        
        thinking_prompt = f"""åŸºäºæ¦‚è¦åˆ†æï¼š
{overview_content}

ç»“åˆæœç´¢ç»“æœï¼š
{search_context}

è¯·å¯¹åŸæ–‡è¿›è¡Œæ·±å…¥æ€è€ƒå’Œåˆ†æï¼š

åŸæ–‡ï¼š
{content}

**åˆ†æä»»åŠ¡ï¼š**
è¯·ç»“åˆæœç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯ï¼Œå¯¹æ–‡ç« è¿›è¡Œæ·±åº¦åˆ†æï¼š

## 1. æ–‡ç« çš„æ·±å±‚ä¸»æ—¨å’Œæ„å›¾
- ç»“åˆèƒŒæ™¯ä¿¡æ¯ç†è§£ä½œè€…çœŸæ­£æƒ³è¡¨è¾¾çš„è§‚ç‚¹
- åˆ†ææ–‡ç« çš„æ·±å±‚å«ä¹‰

## 2. ä½œè€…çš„è§‚ç‚¹ç«‹åœºå’Œä»·å€¼å–å‘
- åŸºäºæœç´¢ç»“æœåˆ†æä½œè€…çš„ç«‹åœº
- è¯†åˆ«å¯èƒ½çš„åè§æˆ–å€¾å‘

## 3. æ–‡ç« çš„ç¤¾ä¼šæ„ä¹‰å’Œå½±å“
- ç»“åˆå½“å‰ç›¸å…³è®¨è®ºåˆ†ææ–‡ç« çš„ç¤¾ä¼šä»·å€¼
- è¯„ä¼°æ–‡ç« å¯èƒ½äº§ç”Ÿçš„å½±å“

## 4. å¯èƒ½çš„äº‰è®®ç‚¹å’Œä¸åŒè§£è¯»
- åŸºäºæœç´¢åˆ°çš„ä¸åŒè§‚ç‚¹åˆ†æäº‰è®®
- æä¾›å¤šè§’åº¦çš„è§£è¯»

## 5. å¯¹è¯»è€…çš„å¯å‘å’Œæ€è€ƒä»·å€¼
- è¯„ä¼°æ–‡ç« çš„æ•™è‚²å’Œå¯å‘æ„ä¹‰
- åˆ†æè¯»è€…å¯ä»¥ä»ä¸­è·å¾—çš„ä»·å€¼

## 6. æ–‡ç« çš„å†å²èƒŒæ™¯å’Œæ—¶ä»£æ„ä¹‰
- ç»“åˆæœç´¢ä¿¡æ¯åˆ†ææ–‡ç« çš„æ—¶ä»£èƒŒæ™¯
- è¯„ä¼°å…¶å†å²æ„ä¹‰å’Œç°å®ä»·å€¼

<think>
æˆ‘éœ€è¦ç»“åˆæœç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯ï¼Œä»æ›´é«˜çš„è§’åº¦æ¥ç†è§£è¿™ç¯‡æ–‡ç« çš„æ·±å±‚å«ä¹‰ï¼ŒåŒ…æ‹¬ä½œè€…çš„çœŸæ­£æ„å›¾ã€ç¤¾ä¼šèƒŒæ™¯ã€ä»·å€¼è§‚å¿µç­‰ã€‚æœç´¢ç»“æœä¸ºæˆ‘æä¾›äº†æ›´å¤šçš„èƒŒæ™¯ä¿¡æ¯å’Œä¸åŒè§†è§’ã€‚
</think>"""
        
        deep_content = ""
        for chunk in ai_service.complex_chat(thinking_prompt):
            thinking, display = ai_service.extract_thinking(chunk)
            if thinking:
                yield f"data: {json.dumps({'type': 'thinking', 'step': 3, 'content': thinking})}\n\n"
            if display:
                deep_content += display
                yield f"data: {json.dumps({'type': 'content', 'step': 3, 'content': display})}\n\n"
        
        yield f"data: {json.dumps({'type': 'step_complete', 'step': 3})}\n\n"
        
        # ç¬¬å››æ­¥ï¼šæ€»ç»“å½’çº³
        yield f"data: {json.dumps({'type': 'step_start', 'step': 4, 'name': 'æ€»ç»“å½’çº³', 'description': 'ç»¼åˆæ‰€æœ‰åˆ†æç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š'})}\n\n"
        
        summary_prompt = f"""è¯·åŸºäºå‰é¢çš„æ‰€æœ‰åˆ†æï¼Œç”Ÿæˆä¸€ä»½å®Œæ•´çš„ç»¼åˆæŠ¥å‘Šï¼š

æ¦‚è¦åˆ†æï¼š
{overview_content}

æœç´¢ç»“æœå’Œç›¸å…³ä¿¡æ¯ï¼š
{search_context}

æ·±å…¥æ€è€ƒï¼š
{deep_content}

åŸæ–‡ï¼š
{content}

**åˆ†æä»»åŠ¡ï¼š**
è¯·ç”Ÿæˆæœ€ç»ˆçš„ç»¼åˆåˆ†ææŠ¥å‘Šï¼š

## 1. æ–‡ç« å…¨é¢æ€»ç»“
- åŸºäºæ¦‚è¦åˆ†æçš„æ–‡ç« æ€»ç»“
- ç»“åˆæœç´¢ä¿¡æ¯çš„èƒŒæ™¯è¡¥å……

## 2. ä¸»è¦å‘ç°å’Œæ´å¯Ÿ
- é€šè¿‡æ·±å…¥åˆ†æå¾—å‡ºçš„å…³é”®å‘ç°
- ç»“åˆå¤–éƒ¨ä¿¡æ¯çš„æ–°æ´å¯Ÿ

## 3. ä»·å€¼è¯„ä¼°å’Œæ„ä¹‰åˆ†æ
- æ–‡ç« çš„å­¦æœ¯æˆ–å®ç”¨ä»·å€¼
- åœ¨ç›¸å…³é¢†åŸŸä¸­çš„åœ°ä½å’Œæ„ä¹‰

## 4. ä¼˜ç¼ºç‚¹è¯„ä»·
- æ–‡ç« çš„ä¼˜ç‚¹å’Œäº®ç‚¹
- å¯èƒ½å­˜åœ¨çš„ä¸è¶³æˆ–å±€é™

## 5. å¯¹è¯»è€…çš„å»ºè®®å’Œå¯å‘
- è¯»è€…åº”è¯¥å…³æ³¨çš„é‡ç‚¹
- å¯ä»¥è·å¾—çš„å¯å‘å’Œæ€è€ƒ

## 6. å»¶ä¼¸æ€è€ƒå’Œç›¸å…³æ¨è
- ç›¸å…³çš„å»¶ä¼¸é˜…è¯»å»ºè®®
- è¿›ä¸€æ­¥æ·±å…¥çš„æ–¹å‘

è¯·ç”¨æ¸…æ™°çš„ç»“æ„å’Œä¸“ä¸šçš„è¯­è¨€å‘ˆç°è¿™ä»½ç»¼åˆæŠ¥å‘Šã€‚

<think>
ç°åœ¨æˆ‘éœ€è¦å°†å‰é¢çš„æ‰€æœ‰åˆ†ææ•´åˆèµ·æ¥ï¼Œå½¢æˆä¸€ä¸ªå®Œæ•´ã€æœ‰æ¡ç†çš„æœ€ç»ˆæŠ¥å‘Šã€‚
</think>"""
        
        for chunk in ai_service.complex_chat(summary_prompt):
            thinking, display = ai_service.extract_thinking(chunk)
            if thinking:
                yield f"data: {json.dumps({'type': 'thinking', 'step': 4, 'content': thinking})}\n\n"
            if display:
                yield f"data: {json.dumps({'type': 'content', 'step': 4, 'content': display})}\n\n"
        
        yield f"data: {json.dumps({'type': 'step_complete', 'step': 4})}\n\n"
        yield f"data: {json.dumps({'type': 'analysis_complete'})}\n\n"
        
    except Exception as e:
        yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"

@comprehensive_analysis_bp.route('/chat', methods=['POST'])
def chat_with_ai():
    """ä¸AIèŠå¤© - å…¨é¢æ€»ç»“åŠŸèƒ½çš„ç»§ç»­å¯¹è¯"""
    try:
        data = request.get_json()
        session_id = data.get('session_id')
        user_message = data.get('message', '').strip()
        chat_history = data.get('chat_history', [])
        
        if not user_message:
            return jsonify({'success': False, 'message': 'æ¶ˆæ¯ä¸èƒ½ä¸ºç©º'})
        
        # æ„å»ºå¯¹è¯å†å²
        messages = []
        for msg in chat_history:
            messages.append({"role": msg['role'], "content": msg['content']})
        
        # æ·»åŠ ç”¨æˆ·æ–°æ¶ˆæ¯
        messages.append({"role": "user", "content": user_message})
        
        return Response(
            generate_chat_response(messages, session_id),
            mimetype='text/plain'
        )
        
    except Exception as e:
        return jsonify({'success': False, 'message': f'èŠå¤©å¤±è´¥ï¼š{str(e)}'})

def generate_chat_response(messages, session_id):
    """ç”ŸæˆèŠå¤©å“åº”"""
    try:
        yield f"data: {json.dumps({'type': 'session_id', 'session_id': session_id})}\n\n"
        
        # ç›´æ¥ä½¿ç”¨å·²æ„å»ºçš„æ¶ˆæ¯åˆ—è¡¨ï¼Œä¸æ·»åŠ é¢å¤–çš„ç©ºæ¶ˆæ¯
        model = ai_service.simple_model  # ä½¿ç”¨ç®€å•æ¨¡å‹è¿›è¡Œå¯¹è¯
        
        for chunk in ai_service._make_request(messages, model):
            # æå–æ€è€ƒå†…å®¹å’Œæ˜¾ç¤ºå†…å®¹
            thinking, display = ai_service.extract_thinking(chunk)
            
            if thinking:
                yield f"data: {json.dumps({'type': 'thinking', 'content': thinking})}\n\n"
            
            if display:
                yield f"data: {json.dumps({'type': 'content', 'content': display})}\n\n"
        
        yield f"data: {json.dumps({'type': 'done'})}\n\n"
        
    except Exception as e:
        yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"
