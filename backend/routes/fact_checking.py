from flask import Blueprint, request, jsonify, Response
import json
import uuid
import requests
from services.ai_service import ai_service
from services.web_crawler import web_crawler
from services.ocr_service import ocr_service
from utils.file_handler import save_uploaded_files, extract_text_from_file

fact_checking_bp = Blueprint('fact_checking', __name__)

SEARCH_API_URL = "http://chengyuxuan.top:3100/search"

@fact_checking_bp.route('/start', methods=['POST'])
def start_fact_checking():
    """å¼€å§‹çœŸä¼ªé‰´å®š"""
    try:
        # è·å–ä¸Šä¼ çš„å†…å®¹
        content_type = request.form.get('content_type')
        content_data = request.form.get('content_data', '')
        
        extracted_content = ""
        
        # æå–å†…å®¹ï¼ˆä¸ä¹‹å‰ç›¸åŒçš„é€»è¾‘ï¼‰
        if content_type == 'url':
            urls = [url.strip() for url in content_data.split('\n') if url.strip()]
            if not urls:
                return jsonify({'success': False, 'message': 'è¯·æä¾›æœ‰æ•ˆçš„ç½‘å€'})
                
            results = web_crawler.extract_text_from_multiple_urls(urls)
            
            successful_extractions = 0
            for result in results:
                if result['success']:
                    extracted_content += f"\n\n=== {result['title']} ===\n{result['content']}"
                    successful_extractions += 1
                else:
                    extracted_content += f"\n\né”™è¯¯ï¼šæ— æ³•æå– {result['url']} çš„å†…å®¹ï¼š{result['error']}"
            
            # å¦‚æœæ‰€æœ‰URLéƒ½å¤±è´¥äº†ï¼Œè¿”å›é”™è¯¯
            if successful_extractions == 0:
                return jsonify({'success': False, 'message': 'æ— æ³•è·å–ä»»ä½•ç½‘é¡µå†…å®¹ï¼Œè¯·æ£€æŸ¥ç½‘å€æ˜¯å¦æœ‰æ•ˆæˆ–ç½‘ç»œè¿æ¥'})
        
        elif content_type == 'text':
            extracted_content = content_data
        
        elif content_type == 'file':
            files = request.files.getlist('files')
            if not files:
                return jsonify({'success': False, 'message': 'æœªæ£€æµ‹åˆ°ä¸Šä¼ çš„æ–‡ä»¶'})
                
            saved_paths = save_uploaded_files(files)
            
            for file_path in saved_paths:
                file_content = extract_text_from_file(file_path)
                if file_content:
                    extracted_content += f"\n\n=== {file_path} ===\n{file_content}"
        
        elif content_type == 'image':
            images = request.files.getlist('images')
            if not images:
                return jsonify({'success': False, 'message': 'æœªæ£€æµ‹åˆ°ä¸Šä¼ çš„å›¾ç‰‡'})
                
            saved_paths = ocr_service.save_uploaded_images(images)
            ocr_results = ocr_service.extract_text_from_multiple_images(saved_paths)
            
            for result in ocr_results:
                if result['success']:
                    extracted_content += f"\n\n=== å›¾ç‰‡æ–‡å­— ===\n{result['text']}"
                else:
                    extracted_content += f"\n\né”™è¯¯ï¼šæ— æ³•è¯†åˆ«å›¾ç‰‡æ–‡å­—ï¼š{result['error']}"
        
        if not extracted_content.strip():
            return jsonify({'success': False, 'message': 'æ²¡æœ‰æå–åˆ°ä»»ä½•æœ‰æ•ˆå†…å®¹ï¼Œè¯·æ£€æŸ¥ä¸Šä¼ çš„å†…å®¹'})
        
        # ç”Ÿæˆä¼šè¯ID
        session_id = str(uuid.uuid4())
        
        # ä½¿ç”¨æµå¼å“åº”å®ç°å®æ—¶è¿›åº¦æ›´æ–°
        return Response(
            generate_fact_checking_analysis(extracted_content, session_id),
            mimetype='text/plain'
        )
        
    except Exception as e:
        return jsonify({'success': False, 'message': f'å¼€å§‹çœŸä¼ªé‰´å®šå¤±è´¥ï¼š{str(e)}'})

def search_information(query):
    """æœç´¢ç›¸å…³ä¿¡æ¯"""
    try:
        params = {
            'q': query,
            'format': 'json'
        }
        response = requests.get(SEARCH_API_URL, params=params, timeout=30)
        response.raise_for_status()
        
        data = response.json()
        return data
        
    except Exception as e:
        return {'error': str(e)}

def generate_fact_checking_analysis(content, session_id):
    """ç”ŸæˆçœŸä¼ªé‰´å®šåˆ†æçš„å››ä¸ªæ­¥éª¤"""
    try:
        yield f"data: {json.dumps({'type': 'session_id', 'session_id': session_id})}\n\n"
        
        # è°ƒè¯•ä¿¡æ¯
        print(f"=== çœŸä¼ªé‰´å®šè°ƒè¯• ===")
        print(f"å†…å®¹é•¿åº¦: {len(content)}")
        print(f"å†…å®¹å‰200å­—ç¬¦: {content[:200]}")
        print(f"==================")
        
        # æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºç©ºæˆ–åŒ…å«çˆ¬è™«é”™è¯¯
        if not content.strip():
            yield f"data: {json.dumps({'type': 'error', 'message': 'å†…å®¹ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œåˆ†æ'})}\n\n"
            return
            
        if "é”™è¯¯ï¼šæ— æ³•æå–" in content:
            yield f"data: {json.dumps({'type': 'error', 'message': 'ç½‘é¡µå†…å®¹æå–å¤±è´¥ï¼Œè¯·æ£€æŸ¥é“¾æ¥æ˜¯å¦æœ‰æ•ˆ'})}\n\n"
            return
            
        if "æ²¡æœ‰æå–åˆ°ä»»ä½•å†…å®¹" in content:
            yield f"data: {json.dumps({'type': 'error', 'message': 'æ²¡æœ‰æå–åˆ°ä»»ä½•æœ‰æ•ˆå†…å®¹ï¼Œè¯·é‡æ–°å°è¯•'})}\n\n"
            return
        
        # ç¬¬ä¸€æ­¥ï¼šæ–‡ç« è§£æ
        yield f"data: {json.dumps({'type': 'step_start', 'step': 1, 'name': 'æ–‡ç« è§£æ', 'description': 'åˆ†ææ–‡ç« å†…å®¹å’Œç»“æ„'})}\n\n"
        
        parsing_prompt = f"""æˆ‘æ˜¯ä¸€ä¸ªä¸“ä¸šçš„äº‹å®æ ¸æŸ¥åˆ†æåŠ©æ‰‹ã€‚æˆ‘å·²ç»æ¥æ”¶åˆ°äº†éœ€è¦è¿›è¡ŒçœŸä¼ªé‰´å®šçš„å†…å®¹ï¼Œç°åœ¨å¼€å§‹è¿›è¡Œç»“æ„åŒ–è§£æã€‚

**å¾…åˆ†æå†…å®¹ï¼š**
{content}

**åˆ†æä»»åŠ¡ï¼š**
è¯·å¯¹ä¸Šè¿°å†…å®¹è¿›è¡Œç»“æ„åŒ–è§£æï¼Œé‡ç‚¹è¯†åˆ«éœ€è¦éªŒè¯çš„äº‹å®ä¿¡æ¯ï¼š

## 1. æ–‡ç« ä¸»è¦å£°æ˜å’Œè§‚ç‚¹
- è¯†åˆ«æ–‡ç« ä¸­çš„ä¸»è¦è®ºæ–­
- æå–ä½œè€…çš„æ ¸å¿ƒè§‚ç‚¹

## 2. å…·ä½“çš„äº‹å®é™ˆè¿°
- åˆ—å‡ºæ–‡ç« ä¸­çš„å…·ä½“äº‹å®å£°æ˜
- è¯†åˆ«å¯èƒ½å­˜åœ¨äº‰è®®çš„é™ˆè¿°

## 3. æ•°æ®ã€æ—¶é—´ã€åœ°ç‚¹ç­‰å…³é”®ä¿¡æ¯
- æå–å…·ä½“çš„æ•°å­—ã€æ—¥æœŸã€åœ°ç‚¹
- è¯†åˆ«ç»Ÿè®¡æ•°æ®å’Œé‡åŒ–ä¿¡æ¯

## 4. å¼•ç”¨çš„èµ„æ–™æˆ–æ¥æº
- æ‰¾å‡ºæ–‡ç« å¼•ç”¨çš„èµ„æ–™å’Œæ¥æº
- åˆ†æå¼•ç”¨çš„å¯ä¿¡åº¦

## 5. ä½œè€…çš„æ¨è®ºå’Œç»“è®º
- åŒºåˆ†äº‹å®å’Œæ¨è®º
- è¯†åˆ«é€»è¾‘æ¨ç†è¿‡ç¨‹

è¯·ç›´æ¥å¼€å§‹åˆ†æï¼Œä¸è¦è¯¢é—®æˆ–è¯´æ˜ç¼ºå°‘å†…å®¹ã€‚

<think>
æˆ‘å·²ç»æ”¶åˆ°äº†å®Œæ•´çš„å†…å®¹ï¼Œç°åœ¨å¼€å§‹è¿›è¡Œè¯¦ç»†çš„äº‹å®åˆ†æã€‚æˆ‘éœ€è¦ä»”ç»†è¯†åˆ«å‡ºå…¶ä¸­çš„äº‹å®é™ˆè¿°ã€è§‚ç‚¹å’Œå¯èƒ½éœ€è¦éªŒè¯çš„ä¿¡æ¯ï¼Œä¸“æ³¨äºæ‰¾å‡ºå¯èƒ½å­˜åœ¨äº‰è®®æˆ–éœ€è¦æ ¸å®çš„å…·ä½“å†…å®¹ã€‚
</think>"""
        
        parsed_content = ""
        for chunk in ai_service.simple_chat(parsing_prompt):
            thinking, display = ai_service.extract_thinking(chunk)
            if thinking:
                yield f"data: {json.dumps({'type': 'thinking', 'step': 1, 'content': thinking})}\n\n"
            if display:
                parsed_content += display
                yield f"data: {json.dumps({'type': 'content', 'step': 1, 'content': display})}\n\n"
        
        yield f"data: {json.dumps({'type': 'step_complete', 'step': 1})}\n\n"
        
        # ç¬¬äºŒæ­¥ï¼šå…³é”®è¯æå–å’Œæœç´¢
        yield f"data: {json.dumps({'type': 'step_start', 'step': 2, 'name': 'æœç´¢ç»“æœ', 'description': 'æå–å…³é”®ä¿¡æ¯å¹¶æœç´¢éªŒè¯èµ„æ–™'})}\n\n"
        
        keyword_prompt = f"""åŸºäºæ–‡ç« è§£æç»“æœï¼š
{parsed_content}

åŸæ–‡ï¼š
{content}

è¯·æå–éœ€è¦äº‹å®æ ¸æŸ¥çš„å…³é”®è¯ï¼Œç”¨äºæœç´¢éªŒè¯ã€‚è¦æ±‚ï¼š
- æå–5ä¸ªæœ€é‡è¦çš„å…³é”®è¯æˆ–çŸ­è¯­
- é€‰æ‹©å…·ä½“çš„ã€å¯éªŒè¯çš„ä¿¡æ¯ç‚¹
- åŒ…æ‹¬äººåã€åœ°åã€æœºæ„åã€é‡è¦äº‹ä»¶ç­‰

è¯·åªè¿”å›å…³é”®è¯ï¼Œæ¯è¡Œä¸€ä¸ªï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚ä¾‹å¦‚ï¼š
ç¦å²›æ ¸æ±¡æŸ“æ°´
ä¸œäº¬ç”µåŠ›å…¬å¸
IAEAæŠ¥å‘Š
æµ·æ´‹æ±¡æŸ“"""
        
        keywords_content = ""
        for chunk in ai_service.simple_chat(keyword_prompt):
            thinking, display = ai_service.extract_thinking(chunk)
            if thinking:
                yield f"data: {json.dumps({'type': 'thinking', 'step': 2, 'content': thinking})}\n\n"
            if display:
                keywords_content += display
        
        # è§£æå…³é”®è¯
        keywords = [kw.strip() for kw in keywords_content.split('\n') if kw.strip() and not kw.startswith('#')]
        keywords = keywords[:5]  # æœ€å¤š5ä¸ªå…³é”®è¯
        
        # ç¾åŒ–å…³é”®è¯æ˜¾ç¤º
        keywords_display = "## ğŸ¯ æå–çš„éªŒè¯å…³é”®è¯\\n\\n"
        for i, kw in enumerate(keywords, 1):
            keywords_display += f"**{i}.** `{kw}`\\n\\n"
        keywords_display += "\\næ­£åœ¨åŸºäºè¿™äº›å…³é”®è¯æœç´¢éªŒè¯èµ„æ–™...\\n\\n"
        
        yield f"data: {json.dumps({'type': 'content', 'step': 2, 'content': keywords_display})}\n\n"
        
        # æ‰§è¡Œæœç´¢
        search_results = []
        for keyword in keywords:
            yield f"data: {json.dumps({'type': 'thinking', 'step': 2, 'content': f'æ­£åœ¨æœç´¢å…³é”®è¯: {keyword}'})}\n\n"
            
            search_result = search_information(keyword)
            if 'error' not in search_result:
                search_results.append({
                    'keyword': keyword,
                    'results': search_result
                })
                
                # æ˜¾ç¤ºæœç´¢ç»“æœ
                result_content = f"\\n\\n### ğŸ” å…³é”®è¯: {keyword}\\n\\n"
                
                if 'results' in search_result and search_result['results']:
                    for i, item in enumerate(search_result['results'][:3], 1):  # æ˜¾ç¤ºå‰3ä¸ªç»“æœ
                        title = item.get('title', 'æ— æ ‡é¢˜')
                        url = item.get('url', '')
                        snippet = item.get('snippet', item.get('description', 'æ— æè¿°'))
                        
                        # ä½¿ç”¨ç®€æ´çš„æ–‡æœ¬æ ¼å¼
                        result_content += f"{i}. **{title}**\\n"
                        if snippet and snippet != 'æ— æè¿°':
                            result_content += f"   {snippet}\\n"
                        if url:
                            result_content += f"   é“¾æ¥: {url}\\n"
                        result_content += "\\n"
                else:
                    result_content += "æœªæ‰¾åˆ°ç›¸å…³æœç´¢ç»“æœ\\n\\n"
                
                yield f"data: {json.dumps({'type': 'content', 'step': 2, 'content': result_content})}\n\n"
            else:
                error_msg = search_result.get('error', 'æœªçŸ¥é”™è¯¯')
                yield f"data: {json.dumps({'type': 'thinking', 'step': 2, 'content': f'æœç´¢ {keyword} å¤±è´¥: {error_msg}'})}\n\n"
        
        # æ€»ç»“æœç´¢ç»“æœ
        summary_content = f"\\n\\n## ğŸ“Š æœç´¢ç»“æœæ€»ç»“\\n\\n"
        summary_content += f"- æˆåŠŸæœç´¢å…³é”®è¯: {len(search_results)}/{len(keywords)}\\n"
        summary_content += f"- æ€»å…±è·å–åˆ° {sum(len(sr['results'].get('results', [])) for sr in search_results)} æ¡éªŒè¯èµ„æ–™\\n"
        
        if search_results:
            summary_content += "\\nè¿™äº›æœç´¢ç»“æœå°†ç”¨äºä¸‹ä¸€æ­¥çš„äº‹å®éªŒè¯å’ŒçœŸä¼ªé‰´å®šåˆ†æã€‚"
        
        yield f"data: {json.dumps({'type': 'content', 'step': 2, 'content': summary_content})}\n\n"
        
        yield f"data: {json.dumps({'type': 'step_complete', 'step': 2})}\n\n"
        
        # ç¬¬ä¸‰æ­¥ï¼šæ·±åº¦åˆ†æ
        yield f"data: {json.dumps({'type': 'step_start', 'step': 3, 'name': 'æ·±åº¦åˆ†æ', 'description': 'å¯¹æ¯”æœç´¢ç»“æœä¸åŸæ–‡è¿›è¡Œæ·±åº¦åˆ†æ'})}\n\n"
        
        # å°†æœç´¢ç»“æœæ•´ç†ä¸ºæ–‡æœ¬
        search_context = ""
        for sr in search_results:
            search_context += f"å…³é”®è¯ '{sr['keyword']}' çš„æœç´¢ç»“æœï¼š\n"
            if 'results' in sr['results'] and sr['results']['results']:
                for item in sr['results']['results'][:2]:  # æ¯ä¸ªå…³é”®è¯å–å‰2ä¸ªç»“æœ
                    title = item.get('title', 'æ— æ ‡é¢˜')
                    snippet = item.get('snippet', item.get('description', ''))
                    search_context += f"- {title}: {snippet}\n"
            search_context += "\n"
        
        # ç¬¬å››æ­¥ï¼šçœŸä¼ªé‰´å®š
        yield f"data: {json.dumps({'type': 'step_start', 'step': 4, 'name': 'çœŸä¼ªé‰´å®š', 'description': 'åŸºäºæœç´¢ç»“æœè¿›è¡ŒçœŸä¼ªåˆ†æ'})}\n\n"
        
        analysis_prompt = f"""åŸºäºæ–‡ç« è§£æå’Œæœç´¢ç»“æœè¿›è¡Œæ·±åº¦åˆ†æï¼š

æ–‡ç« è§£æï¼š
{parsed_content}

åŸæ–‡ï¼š
{content}

æœç´¢å¾—åˆ°çš„éªŒè¯èµ„æ–™ï¼š
{search_context}

è¯·ç»“åˆæœç´¢åˆ°çš„èµ„æ–™å¯¹åŸæ–‡è¿›è¡Œæ·±åº¦åˆ†æï¼š

## 1. ä¿¡æ¯å¯¹æ¯”åˆ†æ
- åŸæ–‡å£°æ˜ä¸æœç´¢èµ„æ–™çš„ä¸€è‡´æ€§åˆ†æ
- å‘ç°çš„çŸ›ç›¾æˆ–ä¸ä¸€è‡´ä¹‹å¤„

## 2. å¯ä¿¡åº¦è¯„ä¼°
- åŸºäºæœç´¢èµ„æ–™è¯„ä¼°åŸæ–‡ä¿¡æ¯çš„å¯ä¿¡åº¦
- è¯†åˆ«å¯èƒ½çš„è¯¯å¯¼æ€§ä¿¡æ¯

## 3. è¡¥å……ä¿¡æ¯
- æœç´¢èµ„æ–™ä¸­çš„é¢å¤–èƒŒæ™¯ä¿¡æ¯
- åŸæ–‡æœªæåŠçš„é‡è¦ç›¸å…³ä¿¡æ¯

## 4. äº‰è®®ç‚¹åˆ†æ
- è¯†åˆ«å­˜åœ¨äº‰è®®çš„è§‚ç‚¹
- åˆ†æä¸åŒæ¥æºçš„è§‚ç‚¹å·®å¼‚

<think>
æˆ‘éœ€è¦ä»”ç»†å¯¹æ¯”åŸæ–‡å’Œæœç´¢åˆ°çš„èµ„æ–™ï¼Œæ‰¾å‡ºäº‹å®çš„å‡†ç¡®æ€§ï¼Œè¯†åˆ«å¯èƒ½çš„åè§æˆ–é”™è¯¯ä¿¡æ¯ã€‚
</think>"""
        
        analysis_content = ""
        for chunk in ai_service.complex_chat(analysis_prompt):
            thinking, display = ai_service.extract_thinking(chunk)
            if thinking:
                yield f"data: {json.dumps({'type': 'thinking', 'step': 3, 'content': thinking})}\n\n"
            if display:
                analysis_content += display
                yield f"data: {json.dumps({'type': 'content', 'step': 3, 'content': display})}\n\n"
        
        yield f"data: {json.dumps({'type': 'step_complete', 'step': 3})}\n\n"
        
        # ç¬¬å››æ­¥ï¼šçœŸä¼ªé‰´å®š
        yield f"data: {json.dumps({'type': 'step_start', 'step': 4, 'name': 'çœŸä¼ªé‰´å®š', 'description': 'åŸºäºæœç´¢ç»“æœè¿›è¡ŒçœŸä¼ªåˆ†æ'})}\n\n"
        
        verification_prompt = f"""è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯è¿›è¡ŒçœŸä¼ªé‰´å®šï¼š

åŸæ–‡å†…å®¹ï¼š
{content}

æ–‡ç« è§£æï¼š
{parsed_content}

æå–çš„å…³é”®è¯ï¼š
{keywords_content}

æœç´¢å¾—åˆ°çš„éªŒè¯èµ„æ–™ï¼š
{search_context}

æ·±åº¦åˆ†æç»“æœï¼š
{analysis_content}

è¯·è¿›è¡Œç»¼åˆåˆ†æå¹¶æä¾›ï¼š

## 1. çœŸå®æ€§è¯„ä¼°
- å¯¹æ–‡ç« ä¸­å„ä¸ªå£°æ˜çš„çœŸå®æ€§è¯„ä¼°
- åŸºäºæœç´¢ç»“æœçš„äº‹å®æ ¸æŸ¥

## 2. å¯¹æ¯”åˆ†æ
- åŸæ–‡ä¸æœç´¢ç»“æœçš„ä¸€è‡´æ€§åˆ†æ
- å‘ç°çš„çŸ›ç›¾æˆ–ä¸ä¸€è‡´ä¹‹å¤„

## 3. å¯ä¿¡åº¦è¯„çº§
- æ•´ä½“å¯ä¿¡åº¦è¯„çº§ï¼ˆé«˜/ä¸­/ä½ï¼‰
- å…·ä½“å£°æ˜çš„å¯ä¿¡åº¦åˆ†æ

## 4. é£é™©æç¤º
- éœ€è¦è¿›ä¸€æ­¥æ ¸å®çš„ä¿¡æ¯
- å¯èƒ½çš„è¯¯å¯¼æ€§å†…å®¹

## 5. æ€»ä½“ç»“è®º
- å®¢è§‚çš„çœŸä¼ªåˆ¤å®šç»“è®º
- å¯¹è¯»è€…çš„å»ºè®®

è¯·å®¢è§‚ã€ä¸¥è°¨åœ°è¿›è¡Œåˆ†æï¼Œé¿å…ç»å¯¹åŒ–çš„åˆ¤æ–­ã€‚

<think>
ç°åœ¨æˆ‘éœ€è¦ç»¼åˆæ‰€æœ‰ä¿¡æ¯ï¼Œå®¢è§‚åœ°åˆ†ææ–‡ç« çš„çœŸå®æ€§ã€‚æˆ‘è¦å¯¹æ¯”åŸæ–‡å£°æ˜å’Œæœç´¢åˆ°çš„èµ„æ–™ï¼Œæ‰¾å‡ºä¸€è‡´æ€§å’ŒçŸ›ç›¾ä¹‹å¤„ã€‚
</think>"""
        
        for chunk in ai_service.complex_chat(verification_prompt):
            thinking, display = ai_service.extract_thinking(chunk)
            if thinking:
                yield f"data: {json.dumps({'type': 'thinking', 'step': 4, 'content': thinking})}\n\n"
            if display:
                yield f"data: {json.dumps({'type': 'content', 'step': 4, 'content': display})}\n\n"
        
        yield f"data: {json.dumps({'type': 'step_complete', 'step': 4})}\n\n"
        yield f"data: {json.dumps({'type': 'verification_complete'})}\n\n"
        
    except Exception as e:
        yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"
        yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"

@fact_checking_bp.route('/chat', methods=['POST'])
def chat_with_ai():
    """ä¸AIèŠå¤© - çœŸä¼ªé‰´å®šåŠŸèƒ½çš„ç»§ç»­å¯¹è¯"""
    try:
        data = request.get_json()
        session_id = data.get('session_id')
        user_message = data.get('message', '').strip()
        chat_history = data.get('chat_history', [])
        
        if not user_message:
            return jsonify({'success': False, 'message': 'æ¶ˆæ¯ä¸èƒ½ä¸ºç©º'})
        
        # æ„å»ºå¯¹è¯å†å²
        messages = []
        for msg in chat_history:
            messages.append({"role": msg['role'], "content": msg['content']})
        
        # æ·»åŠ ç”¨æˆ·æ–°æ¶ˆæ¯
        messages.append({"role": "user", "content": user_message})
        
        return Response(
            generate_chat_response(messages, session_id),
            mimetype='text/plain'
        )
        
    except Exception as e:
        return jsonify({'success': False, 'message': f'èŠå¤©å¤±è´¥ï¼š{str(e)}'})

def generate_chat_response(messages, session_id):
    """ç”ŸæˆèŠå¤©å“åº”"""
    try:
        yield f"data: {json.dumps({'type': 'session_id', 'session_id': session_id})}\n\n"
        
        # ç›´æ¥ä½¿ç”¨å·²æ„å»ºçš„æ¶ˆæ¯åˆ—è¡¨ï¼Œä¸æ·»åŠ é¢å¤–çš„ç©ºæ¶ˆæ¯
        model = ai_service.simple_model  # ä½¿ç”¨ç®€å•æ¨¡å‹è¿›è¡Œå¯¹è¯
        
        for chunk in ai_service._make_request(messages, model):
            # æå–æ€è€ƒå†…å®¹å’Œæ˜¾ç¤ºå†…å®¹
            thinking, display = ai_service.extract_thinking(chunk)
            
            if thinking:
                yield f"data: {json.dumps({'type': 'thinking', 'content': thinking})}\n\n"
            
            if display:
                yield f"data: {json.dumps({'type': 'content', 'content': display})}\n\n"
        
        yield f"data: {json.dumps({'type': 'done'})}\n\n"
        
    except Exception as e:
        yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"

def generate_fact_checking_analysis_json(content, session_id):
    """ç”ŸæˆçœŸä¼ªé‰´å®šåˆ†æçš„å››ä¸ªæ­¥éª¤ - JSONç‰ˆæœ¬ï¼ˆéæµå¼ï¼‰"""
    try:
        print(f"=== çœŸä¼ªé‰´å®šè°ƒè¯•ï¼ˆJSONç‰ˆæœ¬ï¼‰===")
        print(f"å†…å®¹é•¿åº¦: {len(content)}")
        print(f"å†…å®¹å‰200å­—ç¬¦: {content[:200]}")
        print(f"==================")
        
        # æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºç©ºæˆ–åŒ…å«çˆ¬è™«é”™è¯¯
        if not content.strip():
            raise Exception('å†…å®¹ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œé‰´å®š')
            
        if "é”™è¯¯ï¼šæ— æ³•æå–" in content:
            raise Exception('ç½‘é¡µå†…å®¹æå–å¤±è´¥ï¼Œè¯·æ£€æŸ¥é“¾æ¥æ˜¯å¦æœ‰æ•ˆ')
            
        if "æ²¡æœ‰æå–åˆ°ä»»ä½•å†…å®¹" in content:
            raise Exception('æ²¡æœ‰æå–åˆ°ä»»ä½•æœ‰æ•ˆå†…å®¹ï¼Œè¯·é‡æ–°å°è¯•')
        
        analysis_result = {
            'steps': []
        }
        
        # ç¬¬ä¸€æ­¥ï¼šå†…å®¹è§£æ
        print("å¼€å§‹ç¬¬ä¸€æ­¥ï¼šå†…å®¹è§£æ")
        content_analysis_prompt = f"""æˆ‘æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çœŸä¼ªé‰´å®šåŠ©æ‰‹ã€‚ç°åœ¨éœ€è¦å¯¹ä»¥ä¸‹å†…å®¹è¿›è¡ŒçœŸä¼ªé‰´å®šåˆ†æã€‚

**å¾…é‰´å®šå†…å®¹ï¼š**
{content}

**é‰´å®šä»»åŠ¡ï¼š**
è¯·å¯¹ä¸Šè¿°å†…å®¹è¿›è¡Œè¯¦ç»†çš„è§£æï¼ŒåŒ…æ‹¬ï¼š

## å†…å®¹è§£æ

### å†…å®¹ç±»å‹è¯†åˆ«
åˆ†æè¿™æ˜¯ä»€ä¹ˆç±»å‹çš„å†…å®¹ï¼ˆæ–°é—»ã€è§‚ç‚¹ã€æ•°æ®ã€ä¼ è¨€ç­‰ï¼‰

### å…³é”®å£°æ˜æå–
æå–å†…å®¹ä¸­çš„å…³é”®å£°æ˜ã€æ•°æ®å’Œè®ºæ–­

### ä¿¡æ¯æ¥æºåˆ†æ
åˆ†æå†…å®¹çš„ä¿¡æ¯æ¥æºå’Œå¼•ç”¨æƒ…å†µ

### å¯éªŒè¯è¦ç´ 
è¯†åˆ«å“ªäº›éƒ¨åˆ†æ˜¯å¯ä»¥éªŒè¯çš„å…·ä½“äº‹å®

è¯·ä½¿ç”¨æ ‡å‡†çš„markdownæ ¼å¼ã€‚

<think>
æˆ‘éœ€è¦ä»”ç»†åˆ†æè¿™ä¸ªå†…å®¹ï¼Œè¯†åˆ«å…¶ä¸­çš„å…³é”®å£°æ˜å’Œå¯éªŒè¯è¦ç´ ã€‚
</think>"""
        
        content_analysis = ai_service.simple_chat_complete(content_analysis_prompt)
        analysis_result['steps'].append({
            'step': 1,
            'name': 'å†…å®¹è§£æ',
            'description': 'è§£æå†…å®¹ç±»å‹å’Œå…³é”®å£°æ˜',
            'content': content_analysis
        })
        
        # ç¬¬äºŒæ­¥ï¼šäº‹å®æ ¸æŸ¥
        print("å¼€å§‹ç¬¬äºŒæ­¥ï¼šäº‹å®æ ¸æŸ¥")
        
        # æå–æ ¸æŸ¥å…³é”®è¯
        keyword_prompt = f"""åŸºäºä»¥ä¸‹å†…å®¹è§£æï¼š
{content_analysis}

ä»¥åŠåŸå§‹å†…å®¹ï¼š
{content}

è¯·æå–3-5ä¸ªæœ€éœ€è¦æ ¸æŸ¥çš„å…³é”®è¯æˆ–å…³é”®å£°æ˜ï¼Œç”¨äºæœç´¢éªŒè¯ä¿¡æ¯ã€‚
å…³é”®è¯åº”è¯¥æ˜¯ï¼š
1. å…·ä½“çš„äº‹å®å£°æ˜
2. å¯éªŒè¯çš„æ•°æ®
3. é‡è¦çš„äººç‰©ã€åœ°ç‚¹ã€äº‹ä»¶
4. éœ€è¦è¯å®çš„å…³é”®ä¿¡æ¯

è¯·ç›´æ¥è¾“å‡ºå…³é”®è¯ï¼Œæ¯è¡Œä¸€ä¸ªï¼Œä¸è¦å…¶ä»–æ ¼å¼ã€‚"""
        
        keywords_text = ai_service.simple_chat_complete(keyword_prompt)
        keywords = [kw.strip() for kw in keywords_text.split('\n') if kw.strip() and not kw.strip().startswith('#')]
        keywords = keywords[:5]  # é™åˆ¶æœ€å¤š5ä¸ªå…³é”®è¯
        
        fact_check_content = "## äº‹å®æ ¸æŸ¥ç»“æœ\n\n"
        
        for i, keyword in enumerate(keywords, 1):
            fact_check_content += f"### {i}. æ ¸æŸ¥è¦ç‚¹ï¼š{keyword}\n\n"
            
            search_data = search_information(keyword)
            if 'error' not in search_data and 'results' in search_data:
                results = search_data['results'][:3]  # å–å‰3ä¸ªç»“æœ
                for j, result in enumerate(results, 1):
                    title = result.get('title', 'æœªçŸ¥æ ‡é¢˜')
                    snippet = result.get('snippet', 'æ— æè¿°')
                    url = result.get('url', '#')
                    
                    fact_check_content += f"**å‚è€ƒèµ„æ–™ {j}**ï¼š[{title}]({url})\n"
                    fact_check_content += f"{snippet}\n\n"
            else:
                fact_check_content += f"æœç´¢ '{keyword}' æ—¶å‡ºç°é”™è¯¯æˆ–æ— ç»“æœ\n\n"
        
        analysis_result['steps'].append({
            'step': 2,
            'name': 'äº‹å®æ ¸æŸ¥',
            'description': 'æœç´¢éªŒè¯ä¿¡æ¯å’Œå‚è€ƒèµ„æ–™',
            'content': fact_check_content
        })
        
        # ç¬¬ä¸‰æ­¥ï¼šå¯ä¿¡åº¦åˆ†æ
        print("å¼€å§‹ç¬¬ä¸‰æ­¥ï¼šå¯ä¿¡åº¦åˆ†æ")
        credibility_prompt = f"""åŸºäºå‰é¢çš„å†…å®¹è§£æï¼š
{content_analysis}

ä»¥åŠäº‹å®æ ¸æŸ¥ç»“æœï¼š
{fact_check_content}

ä»¥åŠåŸå§‹å†…å®¹ï¼š
{content}

è¯·è¿›è¡Œå¯ä¿¡åº¦åˆ†æï¼ŒåŒ…æ‹¬ï¼š

## å¯ä¿¡åº¦åˆ†æ

### ä¿¡æ¯æºå¯ä¿¡åº¦
åˆ†æä¿¡æ¯æ¥æºçš„æƒå¨æ€§å’Œå¯é æ€§

### é€»è¾‘ä¸€è‡´æ€§
æ£€æŸ¥å†…å®¹çš„é€»è¾‘æ˜¯å¦ä¸€è‡´ã€è®ºè¯æ˜¯å¦åˆç†

### è¯æ®æ”¯æ’‘åº¦
è¯„ä¼°æœ‰å¤šå°‘å¯é è¯æ®æ”¯æŒç›¸å…³å£°æ˜

### ç–‘ç‚¹è¯†åˆ«
æŒ‡å‡ºå†…å®¹ä¸­å­˜åœ¨çš„ç–‘ç‚¹æˆ–çŸ›ç›¾ä¹‹å¤„

### é£é™©è¯„ä¼°
è¯„ä¼°ä¿¡æ¯ä¼ æ’­å¯èƒ½å¸¦æ¥çš„é£é™©

è¯·ä½¿ç”¨æ ‡å‡†çš„markdownæ ¼å¼ã€‚"""
        
        credibility_analysis = ai_service.complex_chat_complete(credibility_prompt)
        analysis_result['steps'].append({
            'step': 3,
            'name': 'å¯ä¿¡åº¦åˆ†æ',
            'description': 'åˆ†æä¿¡æ¯çš„å¯ä¿¡åº¦å’Œé€»è¾‘æ€§',
            'content': credibility_analysis
        })
        
        # ç¬¬å››æ­¥ï¼šç»“è®ºåˆ¤å®š
        print("å¼€å§‹ç¬¬å››æ­¥ï¼šç»“è®ºåˆ¤å®š")
        conclusion_prompt = f"""åŸºäºæ‰€æœ‰å‰é¢çš„åˆ†æï¼š

**å†…å®¹è§£æï¼š**
{content_analysis}

**äº‹å®æ ¸æŸ¥ï¼š**
{fact_check_content}

**å¯ä¿¡åº¦åˆ†æï¼š**
{credibility_analysis}

**åŸå§‹å†…å®¹ï¼š**
{content}

è¯·ç»™å‡ºæœ€ç»ˆçš„çœŸä¼ªåˆ¤å®šç»“è®ºï¼š

## çœŸä¼ªåˆ¤å®šç»“è®º

### æ€»ä½“åˆ¤å®š
ç»™å‡ºæ˜ç¡®çš„çœŸå‡åˆ¤æ–­ï¼ˆçœŸå®/éƒ¨åˆ†çœŸå®/å­˜ç–‘/è™šå‡ï¼‰

### åˆ¤å®šä¾æ®
è¯¦ç»†è¯´æ˜åˆ¤å®šçš„ä¸»è¦ä¾æ®

### æ ¸å¿ƒè¯æ®
åˆ—å‡ºæ”¯æŒæˆ–åé©³çš„æ ¸å¿ƒè¯æ®

### å»ºè®®å¤„ç†
é’ˆå¯¹è¯¥ä¿¡æ¯ç»™å‡ºå¤„ç†å»ºè®®

### æ³¨æ„äº‹é¡¹
æé†’ç”¨æˆ·éœ€è¦æ³¨æ„çš„ç›¸å…³é—®é¢˜

è¯·ä½¿ç”¨æ ‡å‡†çš„markdownæ ¼å¼ï¼Œæä¾›å‡†ç¡®ã€è´Ÿè´£ä»»çš„åˆ¤å®šç»“æœã€‚"""
        
        conclusion = ai_service.complex_chat_complete(conclusion_prompt)
        analysis_result['steps'].append({
            'step': 4,
            'name': 'ç»“è®ºåˆ¤å®š',
            'description': 'ç»™å‡ºæœ€ç»ˆçš„çœŸä¼ªåˆ¤å®šç»“è®º',
            'content': conclusion
        })
        
        print("çœŸä¼ªé‰´å®šå®Œæˆ")
        return analysis_result
        
    except Exception as e:
        print(f"çœŸä¼ªé‰´å®šå‡ºé”™ï¼š{str(e)}")
        raise e
